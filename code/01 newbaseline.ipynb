{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 라이브러리 설치 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    " #!wandb --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wandb login #--relogin\n",
    "# kkukky@naver.com  220215bca12e71dfd5815f1648ca8dbbb2c1bef8\n",
    "#  17b70d1b235684f485db5bcc4b47788ca0e90fd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /data/ephemeral/home/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import pandas as pd             # 데이터 프레임을 다루기 위한 라이브러리입니다. 주로 데이터 처리 및 분석에 사용됩니다.\n",
    "import os                       # 운영 체제와 상호작용하기 위한 모듈로, 파일 및 디렉터리 작업에 사용됩니다.\n",
    "import re                       # 정규 표현식을 사용하여 문자열을 검색, 처리하는 데 사용됩니다.\n",
    "import json                     # JSON 형식의 데이터를 처리하기 위한 라이브러리입니다.\n",
    "import yaml                     # YAML 형식의 데이터를 처리하기 위한 라이브러리입니다.\n",
    "from glob import glob           # 특정 패턴에 맞는 파일 경로들을 리스트로 반환하는 모듈입니다.\n",
    "from tqdm import tqdm           # 반복문에 대한 진행 상황을 시각적으로 보여주는 라이브러리입니다.\n",
    "from pprint import pprint       # 데이터를 좀 더 읽기 쉽게 출력하기 위한 라이브러리입니다.\n",
    "import torch                    # PyTorch 라이브러리로, 딥러닝 모델을 구축하고 학습하기 위한 핵심 라이브러리입니다.\n",
    "import pytorch_lightning as pl  # PyTorch의 고수준 API로, 모델 학습을 간소화하고 구조화된 방식으로 진행할 수 있습니다.\n",
    "from rouge import Rouge         # 텍스트 요약 및 생성 모델의 성능을 평가하기 위해 사용하는 지표 중 하나입니다.\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader  # 데이터셋을 다루고, 이를 모델 학습에 사용할 수 있도록 배치(batch) 단위로 나누는 데 사용됩니다.\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration, BartConfig  # 트랜스포머 모델을 위한 라이브러리로, 토크나이저와 모델을 불러오는 데 사용됩니다.\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer  # Seq2Seq (Sequence-to-Sequence) 모델 학습을 위한 도구와 설정을 제공합니다.\n",
    "from transformers import Trainer, TrainingArguments  # 일반적인 모델 학습을 위한 도구와 설정을 제공합니다.\n",
    "from transformers import EarlyStoppingCallback  # 학습 과정에서 성능 향상이 없을 때 조기 종료를 할 수 있도록 도와주는 콜백 함수입니다.\n",
    "\n",
    "import wandb                    # 모델 학습 과정을 쉽게 추적하고 시각화할 수 있는 툴입니다. 주로 실험 관리 및 결과 기록에 사용됩니다.\n",
    "\n",
    "wandb.login(key=\"220215bca12e71dfd5815f1648ca8dbbb2c1bef8\")\n",
    "\n",
    "#!wandb login #--relogin\n",
    "# kkukky@naver.com     220215bca12e71dfd5815f1648ca8dbbb2c1bef8\n",
    "# kkukky81@gmail.com   17b70d1b235684f485db5bcc4b47788ca0e90fd2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 모델 선택, title 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###    jx7789_kobart_summary_v3-2x75p-5l6-n4-b5    ###\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "### bart 계열\n",
    "# model = \"digit82/kobart-summarization\"\n",
    "# model = \"NLPBada/kobart-chat-persona-extraction-v2\"\n",
    "# model = \"EbanLee/kobart-summary-v3\"\n",
    "model = 'jx7789/kobart_summary_v3'   \n",
    "# model = \"ainize/kobart-news\" 아주 나쁨\n",
    "# model = \"hyesunyun/update-summarization-bart-large-longformer\"\n",
    "\n",
    "### T5 계열\n",
    "# model = \"t5-small\"\n",
    "# model = \"KETI-AIR/ke-t5-base-ko\" 값이 안나옴\n",
    "# model = \"eenzeenee/t5-base-korean-summarization\" 안됨\n",
    "# model = 'psyche/KoT5-summarization' #안됨\n",
    "# model = 'csebuetnlp/mT5_multilingual_XLSum'\n",
    "# lcw99/t5-large-korean-text-summary\n",
    "\n",
    "\n",
    "train = \"train_2x75p.csv\"\n",
    "\n",
    "para = \"-2x75p-5l6-n4-b5\"\n",
    "\n",
    "title = model.replace('/','_') + para\n",
    "\n",
    "# config 설정에 tokenizer 모듈이 사용되므로 미리 tokenizer를 정의해줍니다.\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"digit82/kobart-summarization\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "#from transformers import AutoModel\n",
    "\n",
    "#model = AutoModel.from_pretrained(model, force_download=True)\n",
    "\n",
    "print('###   ', title, '   ###')\n",
    "\n",
    "#output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) config 파일 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = {\n",
    "    \"general\": {\n",
    "        \"data_path\": \"../data/\",  # 모델 학습에 사용할 데이터가 저장된 경로를 지정합니다.\n",
    "        \"model_name\": model,  # 사용할 사전 학습된 모델의 이름을 지정합니다.\n",
    "        \"output_dir\": \"./\"  # 모델의 출력물(예: 생성된 텍스트)을 저장할 디렉터리를 지정합니다.\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"overwrite_output_dir\": True,       # True로 설정하면, 기존에 존재하는 출력 디렉터리 내용을 덮어씁니다.\n",
    "        \"num_train_epochs\": 20,             # 전체 데이터셋을 몇 번 반복해서 학습할지를 설정합니다. (20번)\n",
    "        \"learning_rate\": 5e-6,              # 학습률(learning rate)을 설정합니다.\n",
    "        \"per_device_train_batch_size\": 1,  # 각 디바이스(예: GPU)에서 한 번에 학습할 데이터 배치의 크기를 설정합니다. (50)\n",
    "        \"per_device_eval_batch_size\": 1,   # 평가 시 사용할 배치 크기를 설정합니다. (32)\n",
    "        \"warmup_ratio\": 0.1,                # 학습 초기에 학습률을 천천히 증가시키는 비율을 설정합니다.\n",
    "        \"weight_decay\": 0.01,               # 가중치 감쇠(weight decay) 값을 설정합니다. (과적합 방지를 위해 사용)\n",
    "        \"lr_scheduler_type\": 'cosine_with_restarts',      # 학습률 스케줄러 유형을 'cosine'으로 설정합니다.\n",
    "        \"optim\": 'adamw_torch',             # 옵티마이저(optimizer)로 AdamW를 사용합니다.\n",
    "        \"gradient_accumulation_steps\": 1,   # 기울기(gradient) 축적을 위한 스텝 수를 설정합니다. (1이면 축적 없이 바로 업데이트)\n",
    "        \"evaluation_strategy\": 'epoch',     # 평가를 언제 수행할지 설정합니다. ('epoch'는 각 에폭 종료 시 평가)\n",
    "        \"save_strategy\": 'epoch',           # 모델을 저장할 시점을 설정합니다. ('epoch'는 각 에폭 종료 시 저장)\n",
    "        \"save_total_limit\": 7,              # 최대 저장할 체크포인트 수를 설정합니다. (가장 최근 7개만 유지)\n",
    "        \"fp16\": True,                       # 반정밀도(floating point 16) 연산을 사용할지 설정합니다. (True이면 메모리 절약 및 속도 향상)\n",
    "        \"load_best_model_at_end\": True,     # 학습이 종료될 때 가장 성능이 좋은 모델을 불러옵니다.\n",
    "        \"seed\": 42,                         # 랜덤 시드를 설정하여 실험의 재현성을 보장합니다.\n",
    "        \"logging_dir\": \"./logs\",            # 학습 로그를 저장할 디렉터리를 설정합니다.\n",
    "        \"logging_strategy\": \"epoch\",        # 로그를 언제 기록할지 설정합니다. ('epoch'는 각 에폭 종료 시 기록)\n",
    "        \"predict_with_generate\": True,      # 평가 시 텍스트 생성을 수행할지 설정합니다.\n",
    "        \"generation_max_length\": 200,       # 텍스트 생성 시 최대 길이를 설정합니다. (100 토큰)\n",
    "        \"do_train\": True,                   # 모델을 학습할지 여부를 설정합니다. (True로 설정)\n",
    "        \"do_eval\": True,                    # 모델을 평가할지 여부를 설정합니다. (True로 설정)\n",
    "        \"early_stopping_patience\": 3,       # 조기 종료를 위한 인내 기간(몇 번의 에폭 동안 성능 개선이 없으면 종료)을 설정합니다. (3)\n",
    "        \"early_stopping_threshold\": 0.001,  # 조기 종료를 위한 성능 개선 최소 임계값을 설정합니다. (0.001)\n",
    "        \"report_to\": \"wandb\"                # (선택 사항) wandb를 사용하여 학습 과정을 보고할지 설정합니다.\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"ckt_path\": \"home/code/\",  # 학습된 모델의 체크포인트 파일 경로를 설정합니다.\n",
    "        \"result_path\": \"./prediction/\",  # 추론 결과를 저장할 경로를 설정합니다.\n",
    "        \"no_repeat_ngram_size\": 4,  # 생성된 텍스트에서 동일한 n-gram이 반복되지 않도록 설정합니다. (2-gram 기준)\n",
    "        \"early_stopping\": True,  # 조기 종료를 사용할지 여부를 설정합니다.\n",
    "        \"generate_max_length\": 200,  # 생성 텍스트의 최대 길이를 설정합니다. (100 토큰)\n",
    "        \"num_beams\": 5,  # 빔 서치(beam search) 시 사용할 빔 수를 설정합니다. 1~5까지 가능함. (4)\n",
    "        \"batch_size\": 16,  # 추론 시 사용할 배치 크기를 설정합니다. (32)\n",
    "        # 모델 생성 결과에서 불필요한 토큰들을 제거합니다.\n",
    "        \"remove_tokens\": ['<usr>', f\"{tokenizer.bos_token}\", f\"{tokenizer.eos_token}\", f\"{tokenizer.pad_token}\"]\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"encoder_max_len\": 1024,  # 입력 텍스트를 인코딩할 때 최대 길이를 설정합니다. (512 토큰)\n",
    "        \"decoder_max_len\": 200,  # 출력 텍스트(생성된 텍스트)를 디코딩할 때 최대 길이를 설정합니다. (100 토큰)\n",
    "        \"bos_token\": f\"{tokenizer.bos_token}\",  # 시작 토큰(beginning of sentence)을 지정합니다.\n",
    "        \"eos_token\": f\"{tokenizer.eos_token}\",  # 끝 토큰(end of sentence)을 지정합니다.\n",
    "        # 특정 단어들이 분해되지 않도록, special_tokens을 지정하여 토크나이저에서 처리할 때 이들 단어를 그대로 유지합니다.\n",
    "        \"special_tokens\": [ '#Person1#',     #76737\n",
    "                            '#Person2#',     #70211\n",
    "                            '#Person3#',     #452\n",
    "                            '#Person4#',     #41\n",
    "                            '#Person5#',     #5\n",
    "                            '#Person6#',     #9\n",
    "                            '#Person7#',     #3\n",
    "                            '#SSN#',         #3\n",
    "                            '#PhoneNumber#', #203\n",
    "                            '#Address#',     #45\n",
    "                            '#Email#',       #17\n",
    "                            '#CarNumber#',   #6\n",
    "                            '#CardNumber#'   #10\n",
    "                            '#DateOfBirth#', #8\n",
    "                            '#PassportNumber#']  #7\n",
    "                            # #Person 2#    띄어쓰기 오타 1개 o\n",
    "                            # 9547,9548     #Person1      #없는 오타 2개 o\n",
    "                            # 9547,9548     #Person2      #없는 오타 2개 o\n",
    "                            # 9750,9779     Person1#      #없는 오타 2개 o\n",
    "                            # 420           #PhoneNumber  #없는 오타 1개 o\n",
    "                            # #Person#      숫자 없는 오타 1개 o\n",
    "                            # 839      #사람1만기 시 계정 갱신 o\n",
    "                            # 1033sum  이 사람2#에게 내일 아침 o\n",
    "                            # 1125     사람1#: 제니, 이번 o\n",
    "                            # 1133     사람2#은 그 기간 동 o\n",
    "                            # 1030     이 사람2#에게 내일 o\n",
    "                            # 1142     사람1#: 실례합니다. 저는 o\n",
    "                            # 1199     사람1#은 시험에 대한 준비가 된 o\n",
    "                            # 1213     #하지만 장기간의 o\n",
    "                            # 1236     #고객님, 크루즈 컨트롤에 o\n",
    "                            # 1250     ##여기 있습니다. 스티븐 o\n",
    "                            # 1266     #고객님, 저희는 고객이 화나 o\n",
    "                            # 1278     #고객님, 죄송합니다만 계 o\n",
    "                            # 1281     #잠깐만요, 버전 7 o\n",
    "                            # 1283     #어디 보자. 네, 그런 방 o\n",
    "                            # 1301     #샐러드용 드레싱은 o\n",
    "                            # 1302     #페리에와 짐 빔 세 o\n",
    "                            # 1306     #나 부엌에 있어. . . o\n",
    "                            # 1322     #여기서 만나서 반갑 o\n",
    "                            # 1547     #작은 걸로 주세 o\n",
    "                            # 1609     #여기 있습니다. o\n",
    "                            # 정상 8320   음표는 G#이라고 써있어.\n",
    "                            # 10370    회사 #에서 기술자로 근  o\n",
    "                            # 11716    ##: 안녕, 프란시스   #이 두개 o\n",
    "                            # 4001     (Person A가 탈의실에서 나옴) 스웨터는 어떠셨나요? o\n",
    "                            # 2255     전화번호는 610-555-1234입니다. o\n",
    "                            # 2719     네, 488-6361입니다. 3시까지 저 o\n",
    "                            # 2980     전화번호는 513-3284입니다. o\n",
    "    },\n",
    "    # (선택 사항) wandb 설정: wandb 홈페이지에서 받은 entity, project, run_name 정보를 설정합니다.\n",
    "    \"wandb\": {\n",
    "            \"entity\": \"kkukky-empty\",  # wandb에서 실험을 기록할 엔티티(entity)를 설정합니다.\n",
    "            \"project\": \"NLP2\",  # wandb 프로젝트 이름을 설정합니다.\n",
    "            \"name\": title # wandb에서 실행(run) 이름을 설정합니다.\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "### 변수 적용 간편화를 위해 코드 옮김\n",
    "\n",
    "# 모델의 구성 정보를 YAML 파일로 저장합니다.\n",
    "config_path = \"./config_bart.yaml\"\n",
    "with open(config_path, \"w\") as file:\n",
    "    yaml.dump(config_data, file, allow_unicode=True)\n",
    "\n",
    "# 저장된 config 파일을 불러옵니다.\n",
    "config_path = \"./config_bart.yaml\"\n",
    "with open(config_path, \"r\") as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# 불러온 config 파일의 전체 내용을 확인합니다.\n",
    "# pprint(config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24909</th>\n",
       "      <td>train_12455</td>\n",
       "      <td>#Person1# 실례합니다 . ? 맨체스터 . 출신의 . , 그린 . . 씨이신가...</td>\n",
       "      <td>탄 링은 흰머리와 수염으로 쉽게 인식되는 그린 씨를 만나 호텔로 데려갈 예정입니다....</td>\n",
       "      <td>누군가를 태우다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24910</th>\n",
       "      <td>train_12456</td>\n",
       "      <td>#Person1# 이윙 ? ! 씨가 ; , 우리가 : 컨퍼런스 ; 센터에 ? 오후 ...</td>\n",
       "      <td>#Person1#과 #Person2#는 이윙 씨가 늦지 않도록 요청했기 때문에 컨퍼...</td>\n",
       "      <td>컨퍼런스 센터</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24911</th>\n",
       "      <td>train_12457</td>\n",
       "      <td>#Person1# 오늘 어떻게 , , 도와드릴까요 . !\\n#Person2# 차를 ...</td>\n",
       "      <td>#Person2#는 #Person1#의 도움으로 5일 동안 소형 차를 빌립니다.</td>\n",
       "      <td>차 렌트</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24912</th>\n",
       "      <td>train_12458</td>\n",
       "      <td>#Person1# 오늘 ; . 좀 , 행복해 . 보이지 : 않아 무슨 : 일 ! 있...</td>\n",
       "      <td>#Person2#의 엄마가 일자리를 잃었다. #Person2#는 엄마가 우울해하지 ...</td>\n",
       "      <td>실직</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24913</th>\n",
       "      <td>train_12459</td>\n",
       "      <td>#Person1# 엄마 다음 토요일에 이 , ? 삼촌네 : 가족을 ! 방문하기 , ...</td>\n",
       "      <td>#Person1#은 다음 토요일에 이 삼촌네를 방문할 때 가방을 어떻게 싸야 할지 ...</td>\n",
       "      <td>짐 싸기</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             fname                                           dialogue  \\\n",
       "24909  train_12455  #Person1# 실례합니다 . ? 맨체스터 . 출신의 . , 그린 . . 씨이신가...   \n",
       "24910  train_12456  #Person1# 이윙 ? ! 씨가 ; , 우리가 : 컨퍼런스 ; 센터에 ? 오후 ...   \n",
       "24911  train_12457  #Person1# 오늘 어떻게 , , 도와드릴까요 . !\\n#Person2# 차를 ...   \n",
       "24912  train_12458  #Person1# 오늘 ; . 좀 , 행복해 . 보이지 : 않아 무슨 : 일 ! 있...   \n",
       "24913  train_12459  #Person1# 엄마 다음 토요일에 이 , ? 삼촌네 : 가족을 ! 방문하기 , ...   \n",
       "\n",
       "                                                 summary     topic  \n",
       "24909  탄 링은 흰머리와 수염으로 쉽게 인식되는 그린 씨를 만나 호텔로 데려갈 예정입니다....  누군가를 태우다  \n",
       "24910  #Person1#과 #Person2#는 이윙 씨가 늦지 않도록 요청했기 때문에 컨퍼...   컨퍼런스 센터  \n",
       "24911       #Person2#는 #Person1#의 도움으로 5일 동안 소형 차를 빌립니다.      차 렌트  \n",
       "24912  #Person2#의 엄마가 일자리를 잃었다. #Person2#는 엄마가 우울해하지 ...        실직  \n",
       "24913  #Person1#은 다음 토요일에 이 삼촌네를 방문할 때 가방을 어떻게 싸야 할지 ...      짐 싸기  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config에 저장된 데이터 경로를 통해 train과 validation data를 불러옵니다.\n",
    "data_path = config['general']['data_path']\n",
    "print(data_path)\n",
    "\n",
    "# train data의 구조와 내용을 확인합니다.\n",
    "train_df = pd.read_csv(os.path.join(data_path, train))\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>summary</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>dev_495</td>\n",
       "      <td>#Person1#: 이제 새해가 되어서 새로운 시작을 하려고 결심했어. \\r\\n#P...</td>\n",
       "      <td>#Person1#은 새해에 금연을 하고 커밍아웃하기로 결정했습니다. #Person2...</td>\n",
       "      <td>새해</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>dev_496</td>\n",
       "      <td>#Person1#: 너, 조랑 결혼했지? \\r\\n#Person2#: 조? 무슨 말인...</td>\n",
       "      <td>#Person1#은 #Person2#가 조와 결혼했다고 생각했다. #Person2#...</td>\n",
       "      <td>사랑에 빠지다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>dev_497</td>\n",
       "      <td>#Person1#: 무엇을 도와드릴까요, 부인?\\r\\n#Person2#: 몇 주 동...</td>\n",
       "      <td>#Person2#의 차에서 이상한 소리가 납니다. #Person1#는 브레이크를 교...</td>\n",
       "      <td>소음</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>dev_498</td>\n",
       "      <td>#Person1#: 안녕하세요, 아마존 고객 서비스입니다. 무엇을 도와드릴까요?\\n...</td>\n",
       "      <td>#Person2#님이 아마존 고객 서비스에 전화하여 아마존에서 받은 책에 한 페이지...</td>\n",
       "      <td>빠진 페이지</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>dev_499</td>\n",
       "      <td>#Person1#: 여름이 다 되어간다는 게 믿기지 않아.\\r\\n#Person2#:...</td>\n",
       "      <td>#Person2#는 #Person1#에게 여름 휴가 동안 파티를 도와주는 회사에서 ...</td>\n",
       "      <td>여름 휴가</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fname                                           dialogue  \\\n",
       "494  dev_495  #Person1#: 이제 새해가 되어서 새로운 시작을 하려고 결심했어. \\r\\n#P...   \n",
       "495  dev_496  #Person1#: 너, 조랑 결혼했지? \\r\\n#Person2#: 조? 무슨 말인...   \n",
       "496  dev_497  #Person1#: 무엇을 도와드릴까요, 부인?\\r\\n#Person2#: 몇 주 동...   \n",
       "497  dev_498  #Person1#: 안녕하세요, 아마존 고객 서비스입니다. 무엇을 도와드릴까요?\\n...   \n",
       "498  dev_499  #Person1#: 여름이 다 되어간다는 게 믿기지 않아.\\r\\n#Person2#:...   \n",
       "\n",
       "                                               summary    topic  \n",
       "494  #Person1#은 새해에 금연을 하고 커밍아웃하기로 결정했습니다. #Person2...       새해  \n",
       "495  #Person1#은 #Person2#가 조와 결혼했다고 생각했다. #Person2#...  사랑에 빠지다  \n",
       "496  #Person2#의 차에서 이상한 소리가 납니다. #Person1#는 브레이크를 교...       소음  \n",
       "497  #Person2#님이 아마존 고객 서비스에 전화하여 아마존에서 받은 책에 한 페이지...   빠진 페이지  \n",
       "498  #Person2#는 #Person1#에게 여름 휴가 동안 파티를 도와주는 회사에서 ...    여름 휴가  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation data의 구조와 내용을 확인합니다.\n",
    "val_df = pd.read_csv(os.path.join(data_path,'dev.csv'))\n",
    "val_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>test_495</td>\n",
       "      <td>#Person1#: 헤이, 찰리, 학교 끝나고 우리 집에 와서 나랑 비디오 게임 할...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>test_496</td>\n",
       "      <td>#Person1#: 어떻게 컨트리 음악에 관심을 가지게 되었나요?\\r\\n#Perso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>test_497</td>\n",
       "      <td>#Person1#: 실례합니다, 앨리스. 이곳을 사용해본 적이 없는데, 기계를 어떻...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>test_498</td>\n",
       "      <td>#Person1#: 매튜? 안녕!\\r\\n#Person2#: 스티브! 오랜만이네! 얼...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>test_499</td>\n",
       "      <td>#Person1#: 헤이, 벳시, 좋은 소식 들었어?\\n#Person2#: 아니, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        fname                                           dialogue\n",
       "494  test_495  #Person1#: 헤이, 찰리, 학교 끝나고 우리 집에 와서 나랑 비디오 게임 할...\n",
       "495  test_496  #Person1#: 어떻게 컨트리 음악에 관심을 가지게 되었나요?\\r\\n#Perso...\n",
       "496  test_497  #Person1#: 실례합니다, 앨리스. 이곳을 사용해본 적이 없는데, 기계를 어떻...\n",
       "497  test_498  #Person1#: 매튜? 안녕!\\r\\n#Person2#: 스티브! 오랜만이네! 얼...\n",
       "498  test_499  #Person1#: 헤이, 벳시, 좋은 소식 들었어?\\n#Person2#: 아니, ..."
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validation data의 구조와 내용을 확인합니다.\n",
    "test_df = pd.read_csv(os.path.join(data_path,'test.csv'))\n",
    "test_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 모델 트레이닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- device : cuda:0 ----------\n",
      "2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# 사용할 device를 정의합니다. GPU가 사용 가능하면 'cuda:0', 그렇지 않으면 'cpu'를 사용합니다.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('-' * 10, f'device : {device}', '-' * 10,)\n",
    "print(torch.__version__)\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"사용 중인 GPU: {gpu_name}\")\n",
    "else:\n",
    "    print(\"CUDA를 사용할 수 없습니다. GPU가 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 토크나이저와 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer_and_model_for_train(config, device):\n",
    "    print('-' * 10, 'Load tokenizer & model', '-' * 10,)\n",
    "    print('-' * 10, f'Model Name : {config[\"general\"][\"model_name\"]}', '-' * 10,)\n",
    "\n",
    "     #모델 이름 불러오기:\n",
    "    # config['general']['model_name']을 통해 사전 학습된 모델의 이름을 설정 파일에서 불러옵니다. \n",
    "    # 이는 Hugging Face의 모델 허브에서 가져올 모델의 이름입니다.\n",
    "    model_name = config['general']['model_name']\n",
    "\n",
    "\n",
    "    ### BART ##################\n",
    "    # BART 설정 로드:\n",
    "    # BartConfig를 사용하여 지정된 모델 이름으로부터 BART의 설정을 불러옵니다. \n",
    "    # 이 설정은 모델의 구조와 하이퍼파라미터를 정의합니다.\n",
    "    bart_config = BartConfig().from_pretrained(model_name)\n",
    "\n",
    "    # 토크나이저 로드:\n",
    "    # AutoTokenizer.from_pretrained를 사용하여 지정된 모델 이름으로부터 토크나이저를 불러옵니다. \n",
    "    # 이 토크나이저는 텍스트 데이터를 토큰화하는 역할을 합니다.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # 사전 학습된 모델 로드:\n",
    "    # BartForConditionalGeneration.from_pretrained를 사용하여 사전 학습된 BART 모델을 불러옵니다. \n",
    "    # 이 모델은 텍스트 생성, 요약 등의 작업을 수행하는 데 사용됩니다.\n",
    "    generate_model = BartForConditionalGeneration.from_pretrained(model_name, config=bart_config)\n",
    "\n",
    "\n",
    "    # 특수 토큰 추가:\n",
    "    # special_tokens_dict를 사용하여 설정에서 정의한 특수 토큰을 토크나이저에 추가합니다. \n",
    "    # 이는 예를 들어, 특정 인물이나 장소를 나타내는 토큰을 추가하는 등의 작업에 사용됩니다.\n",
    "    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    # 토크나이저에 추가된 토큰에 맞추어 모델의 토큰 임베딩 크기를 조정합니다.\n",
    "    # 모델의 토큰 임베딩 크기를 업데이트된 토크나이저의 크기에 맞게 조정합니다.\n",
    "    generate_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "    # 모델을 지정된 디바이스로 이동:\n",
    "    # 모델을 device 변수에 지정된 디바이스(GPU 또는 CPU)로 이동시킵니다. \n",
    "    # 이를 통해 모델이 올바른 디바이스에서 실행되도록 합니다.\n",
    "    generate_model.to(device)\n",
    "\n",
    "    \n",
    "    # 모델 설정 출력:\n",
    "    # 로드된 모델의 설정을 출력하여, 모델의 구조와 하이퍼파라미터 등을 확인할 수 있습니다.\n",
    "    print(generate_model.config)\n",
    "\n",
    "    print('-' * 10, 'Load tokenizer & model complete', '-' * 10,)\n",
    "    \n",
    "    # 모델과 토크나이저 반환:\n",
    "    return generate_model, tokenizer\n",
    "    \n",
    "    # 최종적으로 불러온 모델과 토크나이저를 반환하여, 이후 학습 과정에서 사용할 수 있도록 준비합니다.\n",
    "    # 이 함수는 학습을 위해 필요한 모델과 토크나이저를 설정 파일에 따라 자동으로 불러오고 준비해 줍니다. \n",
    "    # 특수 토큰의 추가 및 모델 임베딩의 재구성도 함께 처리하여, 모델이 주어진 작업에 최적화될 수 있도록 돕습니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######  사용할 모델과 토크나이저를 로드합니다.  ################################################################################\n",
      "---------- Load tokenizer & model ----------\n",
      "---------- Model Name : jx7789/kobart_summary_v3 ----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n",
      "/home/.venv/lib/python3.10/site-packages/transformers/modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartConfig {\n",
      "  \"_name_or_path\": \"jx7789/kobart_summary_v3\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"author\": \"Heewon Jeon(madjakarta@gmail.com)\",\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classif_dropout\": 0.1,\n",
      "  \"classifier_dropout\": 0.1,\n",
      "  \"d_model\": 768,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 3072,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 1,\n",
      "  \"do_blenderbot_90_layernorm\": false,\n",
      "  \"dropout\": 0.1,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 3072,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"extra_pos_embeddings\": 2,\n",
      "  \"force_bos_token_to_be_generated\": false,\n",
      "  \"forced_eos_token_id\": 1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"kobart_version\": 2.0,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 64,\n",
      "  \"max_position_embeddings\": 1026,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": 5,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"scale_embedding\": false,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.35.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30046\n",
      "}\n",
      "\n",
      "---------- Load tokenizer & model complete ----------\n",
      "---------- tokenizer special tokens :  {'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>', 'additional_special_tokens': ['#Email#', '#Person5#', '#SSN#', '#CarNumber#', '#PassportNumber#', '#Person2#', '#CardNumber##DateOfBirth#', '#Person7#', '#Address#', '#Person1#', '#Person6#', '#Person3#', '#PhoneNumber#', '#Person4#']} ----------\n"
     ]
    }
   ],
   "source": [
    "# 사용할 모델과 토크나이저를 로드합니다.\n",
    "print(\"\\n#######  사용할 모델과 토크나이저를 로드합니다.  ################################################################################\")\n",
    "generate_model, tokenizer = load_tokenizer_and_model_for_train(config, device)\n",
    "print('-' * 10, \"tokenizer special tokens : \", tokenizer.special_tokens_map, '-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartConfig, BartForConditionalGeneration\n",
    "\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(model)\n",
    "\n",
    "# 모델의 설정(config)을 가져옴\n",
    "bart_config = bart_model.config\n",
    "\n",
    "bart_config\n",
    "\n",
    "\n",
    "# 텍스트 파일 열기 (쓰기 모드)\n",
    "with open('/home/code/config_bart.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(str(bart_config))  # 데이터를 파일에 씁니다.\n",
    "print(\"파일 저장이 완료되었습니다.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터 전처리를 위한 클래스로, 데이터셋을 데이터프레임으로 변환하고 인코더와 디코더의 입력을 생성합니다.\n",
    "class Preprocess:\n",
    "    def __init__(self,\n",
    "            bos_token: str,\n",
    "            eos_token: str,\n",
    "        ) -> None:\n",
    "        \n",
    "        # 클래스 초기화 메서드입니다. 시작 토큰(bos_token)과 종료 토큰(eos_token)을 인스턴스 변수로 저장합니다.\n",
    "        self.bos_token = bos_token\n",
    "        self.eos_token = eos_token\n",
    "\n",
    "    @staticmethod\n",
    "    # 파일 경로를 입력 받아 CSV 파일을 읽고, 필요한 컬럼들만 선택하여 데이터프레임으로 반환합니다.\n",
    "    def make_set_as_df(file_path, is_train = True):\n",
    "\n",
    "        # is_train이 True면 학습용 데이터로, False면 테스트용 데이터로 처리합니다.\n",
    "        if is_train:\n",
    "            df = pd.read_csv(file_path)\n",
    "            train_df = df[['fname','dialogue','summary']]\n",
    "            return train_df\n",
    "\n",
    "        # 테스트 데이터인 경우, 'fname'과 'dialogue' 컬럼만 선택하여 반환합니다.\n",
    "        else:\n",
    "            df = pd.read_csv(file_path)\n",
    "            test_df = df[['fname','dialogue']]\n",
    "            return test_df\n",
    "\n",
    "\n",
    "    # BART 모델의 입력과 출력을 생성하는 메서드입니다.\n",
    "    def make_input(self, dataset, is_test=False):\n",
    "        \n",
    "        # is_test가 True면 테스트 데이터를 위한 입력만 생성하고, False면 학습 데이터를 위한 입력과 출력을 생성합니다.\n",
    "        if is_test:\n",
    "            # 테스트 데이터의 경우, 인코더 입력과 디코더의 시작 토큰으로만 구성된 입력을 반환합니다.\n",
    "            # 인코더 입력으로 'dialogue' 텍스트를 사용합니다.\n",
    "            encoder_input = dataset['dialogue']  \n",
    "            # 디코더 입력은 시작 토큰으로만 구성합니다.\n",
    "            decoder_input = [self.bos_token] * len(dataset['dialogue'])  \n",
    "\n",
    "            return encoder_input.tolist(), list(decoder_input)\n",
    "        else:\n",
    "            # 학습 데이터의 경우, 인코더 입력, 디코더 입력, 디코더 출력을 모두 생성합니다.\n",
    "            encoder_input = dataset['dialogue']  # 인코더 입력으로 'dialogue' 텍스트를 사용합니다.\n",
    "            \n",
    "            # 디코더 입력은 시작 토큰(bos_token)과 요약 텍스트(summary)로 구성됩니다.\n",
    "            decoder_input = dataset['summary'].apply(lambda x: self.bos_token + str(x))\n",
    "            \n",
    "            # 디코더 출력은 요약 텍스트(summary)와 종료 토큰(eos_token)으로 구성됩니다.\n",
    "            decoder_output = dataset['summary'].apply(lambda x: str(x) + self.eos_token)\n",
    "            \n",
    "            # 리스트로 변환하여 반환합니다.\n",
    "            return encoder_input.tolist(), decoder_input.tolist(), decoder_output.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######  학습에 사용할 데이터셋을 전처리하고 로드합니다.  ########################################################################\n"
     ]
    }
   ],
   "source": [
    "# 학습에 사용할 데이터셋을 전처리하고 로드합니다.\n",
    "print(\"\\n#######  학습에 사용할 데이터셋을 전처리하고 로드합니다.  ########################################################################\")\n",
    "# 시작 토큰(beginning of sentence)과 종료 토큰(end of sentence)을 설정합니다.\n",
    "preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 학습 및 검증 데이터셋 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-1) Train, Validation, Test 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train에 사용되는 Dataset 클래스를 정의합니다.\n",
    "class DatasetForTrain(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels, len):\n",
    "        # 학습 데이터셋 초기화 메서드입니다. 인코더 입력, 디코더 입력, 레이블, 데이터 길이를 저장합니다.\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels\n",
    "        self.len = len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스(idx)에 해당하는 데이터를 반환하는 메서드입니다.\n",
    "        \n",
    "        # 인코더 입력 데이터에서 해당 인덱스의 데이터를 복사하여 `item` 딕셔너리에 저장합니다.\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}  # item[input_ids], item[attention_mask]\n",
    "        \n",
    "        # 디코더 입력 데이터에서 해당 인덱스의 데이터를 복사하여 `item2` 딕셔너리에 저장합니다.\n",
    "        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()}  # item2[input_ids], item2[attention_mask]\n",
    "        \n",
    "        # `item2` 딕셔너리의 'input_ids'와 'attention_mask'를 각각 'decoder_input_ids'와 'decoder_attention_mask'로 이름을 변경합니다.\n",
    "        item2['decoder_input_ids'] = item2['input_ids']\n",
    "        item2['decoder_attention_mask'] = item2['attention_mask']\n",
    "        item2.pop('input_ids')  # 'input_ids' 키 제거\n",
    "        item2.pop('attention_mask')  # 'attention_mask' 키 제거\n",
    "        \n",
    "        # `item` 딕셔너리에 디코더의 입력 정보를 추가합니다.\n",
    "        item.update(item2)  # item[input_ids], item[attention_mask], item[decoder_input_ids], item[decoder_attention_mask]\n",
    "        \n",
    "        # 레이블로 사용할 'input_ids' 값을 `item` 딕셔너리에 추가합니다.\n",
    "        item['labels'] = self.labels['input_ids'][idx]  # item[input_ids], item[attention_mask], item[decoder_input_ids], item[decoder_attention_mask], item[labels]\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 길이를 반환하는 메서드입니다.\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# Validation에 사용되는 Dataset 클래스를 정의합니다.\n",
    "class DatasetForVal(Dataset):\n",
    "    def __init__(self, encoder_input, decoder_input, labels, len):\n",
    "        # 검증 데이터셋 초기화 메서드입니다. 학습 데이터셋과 동일한 구조로 정의됩니다.\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.labels = labels\n",
    "        self.len = len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스(idx)에 해당하는 데이터를 반환하는 메서드입니다.\n",
    "        # 학습 데이터셋과 동일하게 정의됩니다.\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()} # item[input_ids], item[attention_mask]\n",
    "        item2 = {key: val[idx].clone().detach() for key, val in self.decoder_input.items()} # item2[input_ids], item2[attention_mask]\n",
    "        item2['decoder_input_ids'] = item2['input_ids']\n",
    "        item2['decoder_attention_mask'] = item2['attention_mask']\n",
    "        item2.pop('input_ids')\n",
    "        item2.pop('attention_mask')\n",
    "        item.update(item2) #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask]\n",
    "        item['labels'] = self.labels['input_ids'][idx] #item[input_ids], item[attention_mask] item[decoder_input_ids], item[decoder_attention_mask], item[labels]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 길이를 반환하는 메서드입니다.\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# Test에 사용되는 Dataset 클래스를 정의합니다.\n",
    "class DatasetForInference(Dataset):     # inference뜻 : 추론\n",
    "    def __init__(self, encoder_input, test_id, len):\n",
    "        # 테스트 데이터셋 초기화 메서드입니다. 인코더 입력, 테스트 ID, 데이터 길이를 저장합니다.\n",
    "        self.encoder_input = encoder_input\n",
    "        self.test_id = test_id\n",
    "        self.len = len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 주어진 인덱스(idx)에 해당하는 데이터를 반환하는 메서드입니다.\n",
    "        # 인코더 입력 데이터에서 해당 인덱스의 데이터를 복사하여 `item` 딕셔너리에 저장합니다.\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encoder_input.items()}\n",
    "        item['ID'] = self.test_id[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 길이를 반환하는 메서드입니다.\n",
    "        return self.len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3-2) Train, Validation 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_dataset(config, preprocessor, data_path, tokenizer):\n",
    "    \n",
    "    ### 데이터 로드 및 변환: \n",
    "    #학습 및 검증 데이터 파일을 읽어 데이터프레임으로 변환합니다.\n",
    "    #데이터프레임으로 변환된 데이터에서 대화(dialogue)와 요약(summary) 텍스트를 각각 학습 입력과 라벨로 사용합니다.\n",
    "    # 데이터셋 경로를 지정합니다.\n",
    "    train_file_path = os.path.join(data_path, train)  # 학습 데이터 파일 경로\n",
    "    val_file_path = os.path.join(data_path, 'dev.csv')  # 검증 데이터 파일 경로\n",
    "    # 학습(train)과 검증(validation) 데이터셋을 데이터프레임으로 변환합니다.\n",
    "    train_data = preprocessor.make_set_as_df(train_file_path)  # 학습 데이터 로드\n",
    "    val_data = preprocessor.make_set_as_df(val_file_path)  # 검증 데이터 로드\n",
    "    # 로드된 학습 데이터와 라벨의 첫 번째 샘플을 출력합니다.\n",
    "    print('-' * 150)\n",
    "    print(f'train_data:\\n {train_data[\"dialogue\"][0]}')\n",
    "    print(f'train_label:\\n {train_data[\"summary\"][0]}')\n",
    "    # 로드된 검증 데이터와 라벨의 첫 번째 샘플을 출력합니다.\n",
    "    print('-' * 150)\n",
    "    print(f'val_data:\\n {val_data[\"dialogue\"][0]}')\n",
    "    print(f'val_label:\\n {val_data[\"summary\"][0]}')\n",
    "\n",
    "\n",
    "    ### 데이터 전처리:\n",
    "    # Preprocess 클래스의 make_input 메서드를 사용하여 인코더 입력, 디코더 입력, 디코더 출력을 생성합니다. \n",
    "    # 이 과정에서 BART 모델에 적합한 형태로 텍스트 데이터를 구성합니다.\n",
    "    # 학습 데이터에 대해 인코더 입력, 디코더 입력, 디코더 출력을 생성합니다.\n",
    "    encoder_input_train, decoder_input_train, decoder_output_train = preprocessor.make_input(train_data)\n",
    "    # 검증 데이터에 대해 인코더 입력, 디코더 입력, 디코더 출력을 생성합니다.\n",
    "    encoder_input_val, decoder_input_val, decoder_output_val = preprocessor.make_input(val_data)\n",
    "    print('-' * 10, 'Load data complete', '-' * 10)\n",
    "\n",
    "\n",
    "    ### 토큰화:\n",
    "    # 텍스트 데이터를 tokenizer를 사용하여 토큰화합니다. 토큰화 과정에서:\n",
    "    # 텍스트를 토큰 ID로 변환합니다.\n",
    "    # 필요한 경우, 패딩(padding=True), 최대 길이(max_length), 특수 토큰(add_special_tokens=True) 등을 추가합니다.\n",
    "    # 반환된 데이터는 PyTorch 텐서(return_tensors=\"pt\")로 반환됩니다.\n",
    "    # 학습 데이터의 인코더 입력을 토크나이저로 토큰화합니다.\n",
    "    tokenized_encoder_inputs = tokenizer(\n",
    "        encoder_input_train, return_tensors=\"pt\", padding=True,\n",
    "        add_special_tokens=True, truncation=True,\n",
    "        max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False\n",
    "    )\n",
    "    # 학습 데이터의 디코더 입력을 토크나이저로 토큰화합니다.\n",
    "    tokenized_decoder_inputs = tokenizer(\n",
    "        decoder_input_train, return_tensors=\"pt\", padding=True,\n",
    "        add_special_tokens=True, truncation=True,\n",
    "        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False\n",
    "    )\n",
    "    # 학습 데이터의 디코더 출력을 토크나이저로 토큰화합니다.\n",
    "    tokenized_decoder_ouputs = tokenizer(\n",
    "        decoder_output_train, return_tensors=\"pt\", padding=True,\n",
    "        add_special_tokens=True, truncation=True,\n",
    "        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "\n",
    "    ### 데이터셋 생성:\n",
    "    # 학습 데이터와 검증 데이터를 각각 DatasetForTrain과 DatasetForVal 클래스로 감싸 데이터셋 객체를 생성합니다.\n",
    "    # 이 객체들은 PyTorch의 DataLoader와 함께 사용되어 모델 학습 및 검증에 활용됩니다.\n",
    "    # 학습용 데이터셋을 생성합니다.\n",
    "    train_inputs_dataset = DatasetForTrain(\n",
    "        tokenized_encoder_inputs, tokenized_decoder_inputs, tokenized_decoder_ouputs, len(encoder_input_train)\n",
    "    )\n",
    "    # 검증 데이터의 인코더 입력을 토크나이저로 토큰화합니다.\n",
    "    val_tokenized_encoder_inputs = tokenizer(\n",
    "        encoder_input_val, return_tensors=\"pt\", padding=True,\n",
    "        add_special_tokens=True, truncation=True,\n",
    "        max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False\n",
    "    )\n",
    "    # 검증 데이터의 디코더 입력을 토크나이저로 토큰화합니다.\n",
    "    val_tokenized_decoder_inputs = tokenizer(\n",
    "        decoder_input_val, return_tensors=\"pt\", padding=True,\n",
    "        add_special_tokens=True, truncation=True,\n",
    "        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False\n",
    "    )\n",
    "    # 검증 데이터의 디코더 출력을 토크나이저로 토큰화합니다.\n",
    "    val_tokenized_decoder_ouputs = tokenizer(\n",
    "        decoder_output_val, return_tensors=\"pt\", padding=True,\n",
    "        add_special_tokens=True, truncation=True,\n",
    "        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False\n",
    "    )\n",
    "    # 검증용 데이터셋을 생성합니다.\n",
    "    val_inputs_dataset = DatasetForVal(\n",
    "        val_tokenized_encoder_inputs, val_tokenized_decoder_inputs, val_tokenized_decoder_ouputs, len(encoder_input_val)\n",
    "    )\n",
    "    print('-' * 10, 'Make dataset complete', '-' * 10)\n",
    "\n",
    "\n",
    "    ### 출력 및 반환:\n",
    "    # 학습용 데이터셋과 검증용 데이터셋을 반환합니다. 이를 통해 모델 학습 과정에서 필요한 데이터를 제공하게 됩니다.\n",
    "    return train_inputs_dataset, val_inputs_dataset\n",
    "\n",
    "    \n",
    "    # 이 함수는 모델 학습 및 검증을 위한 데이터 준비 과정에서 필요한 \n",
    "    # 모든 전처리, 토큰화, 데이터셋 생성을 자동으로 처리하여, 최종적으로 Dataset 객체를 반환합니다. \n",
    "    # 이를 통해 모델 학습 및 검증을 효율적으로 수행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######  학습 및 검증 데이터셋을 준비합니다.  ###################################################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "train_data:\n",
      " #Person1#: 안녕하세요, 스미스씨. 저는 호킨스 의사입니다. 오늘 왜 오셨나요?\n",
      "#Person2#: 건강검진을 받는 것이 좋을 것 같아서요.\n",
      "#Person1#: 그렇군요, 당신은 5년 동안 건강검진을 받지 않았습니다. 매년 받아야 합니다.\n",
      "#Person2#: 알고 있습니다. 하지만 아무 문제가 없다면 왜 의사를 만나러 가야 하나요?\n",
      "#Person1#: 심각한 질병을 피하는 가장 좋은 방법은 이를 조기에 발견하는 것입니다. 그러니 당신의 건강을 위해 최소한 매년 한 번은 오세요.\n",
      "#Person2#: 알겠습니다.\n",
      "#Person1#: 여기 보세요. 당신의 눈과 귀는 괜찮아 보입니다. 깊게 숨을 들이쉬세요. 스미스씨, 담배 피우시나요?\n",
      "#Person2#: 네.\n",
      "#Person1#: 당신도 알다시피, 담배는 폐암과 심장병의 주요 원인입니다. 정말로 끊으셔야 합니다. \n",
      "#Person2#: 수백 번 시도했지만, 습관을 버리는 것이 어렵습니다.\n",
      "#Person1#: 우리는 도움이 될 수 있는 수업과 약물들을 제공하고 있습니다. 나가기 전에 더 많은 정보를 드리겠습니다.\n",
      "#Person2#: 알겠습니다, 감사합니다, 의사선생님.\n",
      "train_label:\n",
      " 스미스씨가 건강검진을 받고 있고, 호킨스 의사는 매년 건강검진을 받는 것을 권장합니다. 호킨스 의사는 스미스씨가 담배를 끊는 데 도움이 될 수 있는 수업과 약물에 대한 정보를 제공할 것입니다.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "val_data:\n",
      " #Person1#: 안녕하세요, 오늘 하루 어떠셨어요? \n",
      "#Person2#: 요즘 숨쉬기가 좀 힘들어요.\n",
      "#Person1#: 최근에 감기 같은 것에 걸리신 적이 있나요?\n",
      "#Person2#: 아니요, 감기는 아니에요. 그냥 숨을 쉴 때마다 가슴이 무겁게 느껴져요.\n",
      "#Person1#: 알고 있는 알레르기가 있나요?\n",
      "#Person2#: 아니요, 알고 있는 알레르기는 없어요.\n",
      "#Person1#: 이런 증상이 항상 나타나나요, 아니면 활동할 때 주로 나타나나요?\n",
      "#Person2#: 운동을 할 때 많이 나타나요.\n",
      "#Person1#: 저는 당신을 폐 전문의에게 보내서 천식에 대한 검사를 받게 할 거예요.\n",
      "#Person2#: 도와주셔서 감사합니다, 의사 선생님.\n",
      "val_label:\n",
      " #Person2#는 숨쉬기에 어려움을 겪는다. 의사는 #Person1#에게 이에 대해 묻고, #Person2#를 폐 전문의에게 보낼 예정이다. \n",
      "---------- Load data complete ----------\n",
      "---------- Make dataset complete ----------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n#######  학습 및 검증 데이터셋을 준비합니다.  ###################################################################################\")\n",
    "# 학습 및 검증 데이터셋을 준비합니다.\n",
    "data_path = config['general']['data_path']  # 데이터 경로를 설정에서 가져옵니다.\n",
    "train_inputs_dataset, val_inputs_dataset = prepare_train_dataset(config, preprocessor, data_path, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Trainer 클래스 초기화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(config, tokenizer, pred):\n",
    "    ### rouge = Rouge():\n",
    "    # ROUGE 점수를 계산하기 위해 Rouge 클래스를 초기화합니다. \n",
    "    # ROUGE는 주로 텍스트 요약의 품질을 평가할 때 사용되는 지표입니다.\n",
    "    rouge = Rouge()\n",
    "\n",
    "\n",
    "    ### pred.predictions 및 pred.label_ids:\n",
    "    # predictions: 모델이 예측한 토큰 ID의 배열입니다.\n",
    "    # label_ids: 실제 레이블(정답) 토큰 ID의 배열입니다.\n",
    "    # 예측된 토큰 ID와 실제 레이블 ID를 가져옵니다.\n",
    "    predictions = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "\n",
    "\n",
    "    ### 패딩 토큰 처리:\n",
    "    # 예측 값과 레이블에서 -100으로 표시된 패딩 토큰을 실제 패딩 토큰 ID로 교체하여 평가에서 패딩이 영향을 미치지 않도록 합니다.\n",
    "    # 모델 출력 중 패딩 토큰을 의미하는 -100 값을 tokenizer의 패딩 토큰 ID로 변경합니다.\n",
    "    predictions[predictions == -100] = tokenizer.pad_token_id\n",
    "    labels[labels == -100] = tokenizer.pad_token_id\n",
    "\n",
    "\n",
    "    ### 토큰 디코딩:\n",
    "    # 토큰 ID 배열을 원래의 텍스트 문자열로 변환합니다.\n",
    "    # batch_decode는 여러 개의 토큰 배열을 한꺼번에 디코딩합니다.\n",
    "    # 예측된 토큰 ID를 텍스트로 디코딩합니다.\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, clean_up_tokenization_spaces=True)\n",
    "    # 실제 레이블의 토큰 ID도 텍스트로 디코딩합니다.\n",
    "    labels = tokenizer.batch_decode(labels, clean_up_tokenization_spaces=True)\n",
    "\n",
    "\n",
    "    ### 불필요한 토큰 제거:\n",
    "    # 모델이 생성한 텍스트에서 사전에 정의된 불필요한 토큰을 제거하여 평가의 정확성을 높입니다.\n",
    "    # 평가를 위해 불필요한 토큰들을 제거합니다.\n",
    "    replaced_predictions = decoded_preds.copy()  # 예측된 텍스트 복사\n",
    "    replaced_labels = labels.copy()  # 실제 레이블 텍스트 복사\n",
    "    remove_tokens = config['inference']['remove_tokens']  # 제거할 토큰 목록을 config에서 가져옵니다.\n",
    "    # 각 불필요한 토큰을 제거합니다.\n",
    "    for token in remove_tokens:\n",
    "        replaced_predictions = [sentence.replace(token, \" \") for sentence in replaced_predictions]\n",
    "        replaced_labels = [sentence.replace(token, \" \") for sentence in replaced_labels]\n",
    "\n",
    "\n",
    "    ### 출력:\n",
    "    # 평가를 위해 디코딩된 예측 텍스트와 실제 레이블의 일부 샘플을 출력합니다.\n",
    "    # 첫 번째, 두 번째, 세 번째 예측과 실제 레이블을 출력합니다.\n",
    "    print('-' * 150)\n",
    "    print(f\"PRED: {replaced_predictions[0]}\")\n",
    "    print(f\"GOLD: {replaced_labels[0]}\")\n",
    "    print('-' * 150)\n",
    "    print(f\"PRED: {replaced_predictions[1]}\")\n",
    "    print(f\"GOLD: {replaced_labels[1]}\")\n",
    "    print('-' * 150)\n",
    "    print(f\"PRED: {replaced_predictions[2]}\")\n",
    "    print(f\"GOLD: {replaced_labels[2]}\")\n",
    "\n",
    "\n",
    "    ### ROUGE 점수 계산:\n",
    "    # replaced_predictions와 replaced_labels를 사용하여 ROUGE 점수를 계산합니다.\n",
    "    # ROUGE-1, ROUGE-2, ROUGE-L 등의 F1 점수를 계산하여 반환합니다.\n",
    "    # 최종적으로 ROUGE 점수를 계산합니다.\n",
    "    results = rouge.get_scores(replaced_predictions, replaced_labels, avg=True)\n",
    "\n",
    "\n",
    "    ### 결과 반환:\n",
    "    # ROUGE 점수 중 F1-score를 추출하여 딕셔너리 형태로 반환합니다.\n",
    "    result = {key: value[\"f\"] for key, value in results.items()}\n",
    "    return result\n",
    "\n",
    "\n",
    "    # 이 함수는 모델이 생성한 텍스트의 품질을 ROUGE 지표로 평가하여, \n",
    "    # 모델 성능을 평가하는 데 중요한 역할을 합니다. \n",
    "    # ROUGE 점수를 통해 텍스트 요약 또는 생성 모델의 정확성을 평가할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset):\n",
    "    print('-' * 10, 'Make training arguments', '-' * 10,)\n",
    "    \n",
    "    ### Seq2SeqTrainingArguments 설정:\n",
    "    # 학습과 관련된 다양한 설정값들을 정의하는 Seq2SeqTrainingArguments 객체를 생성합니다.\n",
    "    # 학습률, 배치 크기, 에포크 수, 로그 저장 위치 등 다양한 하이퍼파라미터와 옵션들이 포함됩니다.\n",
    "    # 학습을 위한 설정값들을 정의합니다.\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config['general']['output_dir'],                                     # 모델 출력 디렉터리\n",
    "        overwrite_output_dir=config['training']['overwrite_output_dir'],                # 출력 디렉터리를 덮어쓸지 여부\n",
    "        num_train_epochs=config['training']['num_train_epochs'],                        # 전체 학습 에포크 수\n",
    "        learning_rate=config['training']['learning_rate'],                              # 학습률\n",
    "        per_device_train_batch_size=config['training']['per_device_train_batch_size'],  # 학습 시 디바이스당 배치 크기\n",
    "        per_device_eval_batch_size=config['training']['per_device_eval_batch_size'],    # 평가 시 디바이스당 배치 크기\n",
    "        warmup_ratio=config['training']['warmup_ratio'],                                # 학습 초기에 학습률을 점진적으로 증가시키는 비율\n",
    "        weight_decay=config['training']['weight_decay'],                                # 가중치 감쇠 (과적합 방지)\n",
    "        lr_scheduler_type=config['training']['lr_scheduler_type'],                      # 학습률 스케줄러 유형\n",
    "        optim=config['training']['optim'],                                              # 옵티마이저 종류\n",
    "        gradient_accumulation_steps=config['training']['gradient_accumulation_steps'],  # 기울기(gradient) 축적 단계 수\n",
    "        evaluation_strategy=config['training']['evaluation_strategy'],                  # 학습 중 평가 전략 (예: 에포크마다 평가)\n",
    "        save_strategy=config['training']['save_strategy'],                              # 모델 저장 전략\n",
    "        save_total_limit=config['training']['save_total_limit'],                        # 저장할 체크포인트의 최대 개수\n",
    "        fp16=config['training']['fp16'],                                                # 반정밀도(float16) 연산 사용 여부\n",
    "        load_best_model_at_end=config['training']['load_best_model_at_end'],            # 학습 종료 시 가장 좋은 모델 로드\n",
    "        seed=config['training']['seed'],                                                # 랜덤 시드 설정\n",
    "        logging_dir=config['training']['logging_dir'],                                  # 로그 저장 디렉터리\n",
    "        logging_strategy=config['training']['logging_strategy'],                        # 로그 기록 전략 (예: 에포크마다 기록)\n",
    "        predict_with_generate=config['training']['predict_with_generate'],              # 텍스트 생성 후 평가 지표를 계산할지 여부\n",
    "        generation_max_length=config['training']['generation_max_length'],              # 텍스트 생성 시 최대 길이\n",
    "        do_train=config['training']['do_train'],                                        # 학습 수행 여부\n",
    "        do_eval=config['training']['do_eval'],                                          # 평가 수행 여부\n",
    "        report_to=config['training']['report_to']                                       # (선택 사항) wandb로 학습 과정을 기록할지 여부\n",
    "    )\n",
    "\n",
    "\n",
    "    ### wandb 초기화 (선택 사항):\n",
    "    # (선택 사항) wandb를 사용하여 학습 과정을 추적할 때 초기화합니다.\n",
    "    # WandB(Weights & Biases)로 학습 과정을 추적하고 시각화하려면 wandb.init()을 사용해 초기화할 수 있습니다.\n",
    "    # 이 부분은 현재 주석 처리되어 있으며, 필요한 경우 활성화할 수 있습니다.\n",
    "    wandb.init(\n",
    "         entity=config['wandb']['entity'],\n",
    "         project=config['wandb']['project'],\n",
    "         name=config['wandb']['name']\n",
    "    )\n",
    "    # (선택 사항) 모델 체크포인트를 wandb에 저장하도록 환경 변수를 설정합니다.\n",
    "    os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "    os.environ[\"WANDB_WATCH\"] = \"false\"\n",
    "    \n",
    "    \n",
    "    ### EarlyStoppingCallback 설정:\n",
    "    # EarlyStoppingCallback을 사용하여 학습이 진행될 때 검증 손실이 더 이상 개선되지 않으면 학습을 조기에 중단시킵니다.\n",
    "    # 설정된 early_stopping_patience와 early_stopping_threshold에 따라 작동합니다.\n",
    "    # EarlyStoppingCallback: 검증 손실이 더 이상 개선되지 않을 때 학습을 중단시키는 콜백을 설정합니다.\n",
    "    MyCallback = EarlyStoppingCallback(\n",
    "        early_stopping_patience=config['training']['early_stopping_patience'],      # 개선이 없을 경우 중단까지 기다릴 에포크 수\n",
    "        early_stopping_threshold=config['training']['early_stopping_threshold']     # 개선으로 간주할 최소 손실 감소량\n",
    "    )\n",
    "    print('-' * 10, 'Make training arguments complete', '-' * 10,)\n",
    "    print('-' * 10, 'Make trainer', '-' * 10,)\n",
    "\n",
    "\n",
    "    ### Seq2SeqTrainer 초기화:\n",
    "    # Seq2SeqTrainer는 Hugging Face의 transformers 라이브러리에서 제공하는 훈련 도구로, \n",
    "    # 시퀀스-투-시퀀스 모델의 학습과 평가를 위한 도구입니다.\n",
    "    # 학습할 모델, 설정값, 학습/검증 데이터셋, 평가 메트릭 함수, 콜백 등을 인자로 받아 초기화합니다.\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=generate_model,  # 학습할 모델\n",
    "        args=training_args,  # 학습 설정값\n",
    "        train_dataset=train_inputs_dataset,  # 학습 데이터셋\n",
    "        eval_dataset=val_inputs_dataset,  # 검증 데이터셋\n",
    "        compute_metrics=lambda pred: compute_metrics(config, tokenizer, pred),  # 성능 평가를 위한 메트릭 함수\n",
    "        callbacks=[MyCallback]  # EarlyStoppingCallback을 포함한 콜백 리스트\n",
    "    )\n",
    "    print('-' * 10, 'Make trainer complete', '-' * 10,)\n",
    "\n",
    "    ### Trainer 객체 반환:\n",
    "    # 생성된 trainer 객체를 반환하여, 이후 학습을 진행할 수 있게 합니다.\n",
    "    return trainer  \n",
    "\n",
    "\n",
    "    # 이 함수는 모델 학습을 시작하기 위한 모든 준비 작업을 자동으로 처리하며, \n",
    "    # 사용자에게 최적화된 Trainer 객체를 제공합니다. 이를 통해 학습 및 평가를 손쉽게 수행할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model.config.max_length = 128\n",
    "generate_model.config.no_repeat_ngram_size = 4\n",
    "generate_model.config.torch_dtype = 'float16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######  학습을 위한 Trainer 클래스를 초기화합니다.  ############################################################################\n",
      "---------- Make training arguments ----------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/code/wandb/run-20240910_020946-ksxam61b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kkukky-empty/NLP2/runs/ksxam61b' target=\"_blank\">jx7789_kobart_summary_v3-2x75p-5l6-n4-b5</a></strong> to <a href='https://wandb.ai/kkukky-empty/NLP2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kkukky-empty/NLP2' target=\"_blank\">https://wandb.ai/kkukky-empty/NLP2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kkukky-empty/NLP2/runs/ksxam61b' target=\"_blank\">https://wandb.ai/kkukky-empty/NLP2/runs/ksxam61b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:457: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "/home/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Make training arguments complete ----------\n",
      "---------- Make trainer ----------\n",
      "---------- Make trainer complete ----------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n#######  학습을 위한 Trainer 클래스를 초기화합니다.  ############################################################################\")\n",
    "# 학습을 위한 Trainer 클래스를 초기화합니다.\n",
    "trainer = load_trainer_for_train(config, generate_model, tokenizer, train_inputs_dataset, val_inputs_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) 모델 학습, wandb 세션종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######  모델 학습을 시작합니다.  ##############################################################################################\n",
      "jx7789_kobart_summary_v3-2x75p-5l6-n4-b5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='344' max='62300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  344/62300 01:02 < 3:09:30, 5.45 it/s, Epoch 0.11/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(title)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 모델 학습을 시작합니다.\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m#######  wandb 세션을 종료합니다.  ############################################################################################\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# (선택) 모델 학습이 완료된 후 wandb 세션을 종료합니다.\u001b[39;00m\n",
      "File \u001b[0;32m/home/.venv/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/.venv/lib/python3.10/site-packages/transformers/trainer.py:1860\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1857\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1859\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1860\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/home/.venv/lib/python3.10/site-packages/transformers/trainer.py:2734\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2733\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2734\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2736\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m/home/.venv/lib/python3.10/site-packages/accelerate/accelerator.py:2192\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2193\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[1;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[0;32m/home/.venv/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/.venv/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n#######  모델 학습을 시작합니다.  ##############################################################################################\")\n",
    "print(title)\n",
    "# 모델 학습을 시작합니다.\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n#######  wandb 세션을 종료합니다.  ############################################################################################\")\n",
    "# (선택) 모델 학습이 완료된 후 wandb 세션을 종료합니다.\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 모델 추론하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이곳에 내가 사용할 wandb config 설정 \"추론에 사용할 ckt 경로 설정\"\n",
    "# loaded_config['inference']['ckt_path'] = \"home/ckt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- device : cuda:0 ----------\n",
      "2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "# 사용할 device를 정의합니다. GPU가 사용 가능하면 'cuda:0', 그렇지 않으면 'cpu'를 사용합니다.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('-' * 10, f'device : {device}', '-' * 10,)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 모델과 토크나이저 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red size=32><b> ckt 값 바꾸기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '멈춤' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43m멈춤\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name '멈춤' is not defined"
     ]
    }
   ],
   "source": [
    "멈춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_num = \"62290\"\n",
    "\n",
    "# 추론을 위한 tokenizer와 학습시킨 모델을 불러옵니다.\n",
    "def load_tokenizer_and_model_for_test(config, device):\n",
    "    print('-' * 10, 'Load tokenizer & model', '-' * 10,)\n",
    "\n",
    "    print(\"\\n######  설정 파일에서 모델 이름과 체크포인트 경로를 불러옵니다.\")\n",
    "    ### 모델 이름과 체크포인트 경로 설정:\n",
    "    # 설정 파일에서 모델 이름과 체크포인트 경로를 불러옵니다.\n",
    "    # config['general']['model_name']: 사전 학습된 모델의 이름을 설정 파일에서 가져옵니다.\n",
    "    # config['inference']['ckt_path']: 학습된 모델의 체크포인트 경로를 설정 파일에서 가져옵니다. 이 경로는 학습이 완료된 후 저장된 모델의 위치를 나타냅니다.\n",
    "    model_name = config['general']['model_name']\n",
    "    ckt_path = config['inference']['ckt_path']  # 학습된 모델의 체크포인트 경로\n",
    "    ckt_path = \"/home/code/checkpoint-\" + checkpoint_num\n",
    "    print(model_name, \" / \", ckt_path)\n",
    "    print('-' * 10, f'Model Name : {model_name}', '-' * 10,)\n",
    "\n",
    "\n",
    "    ### 토크나이저 로드 및 특수 토큰 추가:\n",
    "    # AutoTokenizer.from_pretrained(model_name): 사전 학습된 모델 이름을 사용하여 토크나이저를 로드합니다.\n",
    "    # tokenizer.add_special_tokens(special_tokens_dict): 설정 파일에서 정의된 특수 토큰을 토크나이저에 추가합니다. 이는 모델이 특정 단어들을 특수 토큰으로 처리할 수 있도록 합니다.\n",
    "    print(\"\\n######  지정된 모델 이름으로부터 토크나이저를 로드합니다.\")\n",
    "    # 지정된 모델 이름으로부터 토크나이저를 로드합니다.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    print(\"\\n######  설정 파일에서 정의된 특수 토큰을 토크나이저에 추가합니다.\")\n",
    "    # 설정 파일에서 정의된 특수 토큰을 토크나이저에 추가합니다.\n",
    "    special_tokens_dict = {'additional_special_tokens': config['tokenizer']['special_tokens']}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "\n",
    "    ### 모델 로드:\n",
    "    # BartForConditionalGeneration.from_pretrained(ckt_path): 학습된 모델의 체크포인트에서 BART 모델을 로드합니다.\n",
    "    # generate_model.resize_token_embeddings(len(tokenizer)): 토크나이저에 추가된 특수 토큰에 맞춰 모델의 토큰 임베딩 크기를 재조정합니다.\n",
    "    print(\"\\n######  학습된 체크포인트에서 BART 모델을 로드합니다.\")\n",
    "    # 학습된 체크포인트에서 BART 모델을 로드합니다.\n",
    "    generate_model = BartForConditionalGeneration.from_pretrained(ckt_path)\n",
    "    print(\"\\n######  추가된 토큰에 맞게 모델의 토큰 임베딩 크기를 조정합니다.\")\n",
    "    # 추가된 토큰에 맞게 모델의 토큰 임베딩 크기를 조정합니다.\n",
    "    generate_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "    ### 디바이스로 모델 이동:\n",
    "    # generate_model.to(device): 로드된 모델을 지정된 디바이스(GPU 또는 CPU)로 이동시켜, \n",
    "    # 추론 작업을 해당 디바이스에서 수행할 수 있도록 합니다.\n",
    "    print(\"\\n######  모델을 지정된 디바이스(GPU 또는 CPU)로 이동시킵니다.\")\n",
    "    # 모델을 지정된 디바이스(GPU 또는 CPU)로 이동시킵니다.\n",
    "    generate_model.to(device)\n",
    "    \n",
    "    print('-' * 10, 'Load tokenizer & model complete', '-' * 10,)\n",
    "\n",
    "\n",
    "    ### 모델과 토크나이저 반환:\n",
    "    # 최종적으로 불러온 모델과 토크나이저를 반환하여, 이후 추론 작업에서 사용할 수 있도록 합니다.\n",
    "    return generate_model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######  추론을 위한 모델과 토크나이저를 불러옵니다.  ##############################################################################\n",
      "---------- Load tokenizer & model ----------\n",
      "\n",
      "######  설정 파일에서 모델 이름과 체크포인트 경로를 불러옵니다.\n",
      "jx7789/kobart_summary_v3  /  /home/code/checkpoint-62290\n",
      "---------- Model Name : jx7789/kobart_summary_v3 ----------\n",
      "\n",
      "######  지정된 모델 이름으로부터 토크나이저를 로드합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You passed along `num_labels=3` with an incompatible id to label map: {'0': 'NEGATIVE', '1': 'POSITIVE'}. The number of labels wil be overwritten to 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######  설정 파일에서 정의된 특수 토큰을 토크나이저에 추가합니다.\n",
      "\n",
      "######  학습된 체크포인트에서 BART 모델을 로드합니다.\n",
      "\n",
      "######  추가된 토큰에 맞게 모델의 토큰 임베딩 크기를 조정합니다.\n",
      "\n",
      "######  모델을 지정된 디바이스(GPU 또는 CPU)로 이동시킵니다.\n",
      "---------- Load tokenizer & model complete ----------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n#######  추론을 위한 모델과 토크나이저를 불러옵니다.  ##############################################################################\")\n",
    "# 추론을 위한 모델과 토크나이저를 불러옵니다.\n",
    "generate_model, tokenizer = load_tokenizer_and_model_for_test(config, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######  데이터 경로와 전처리기를 설정합니다.  ####################################################################################\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n#######  데이터 경로와 전처리기를 설정합니다.  ####################################################################################\")\n",
    "# 데이터 경로와 전처리기를 설정합니다.\n",
    "data_path = config['general']['data_path']\n",
    "preprocessor = Preprocess(config['tokenizer']['bos_token'], config['tokenizer']['eos_token'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 테스트 데이터셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization 과정까지 진행된 최종적으로 모델에 입력될 데이터를 출력합니다.\n",
    "def prepare_test_dataset(config, preprocessor, tokenizer):\n",
    "    \n",
    "    ### 테스트 데이터 로드:\n",
    "    # test_file_path에서 test.csv 파일을 읽어옵니다.\n",
    "    test_file_path = os.path.join(config['general']['data_path'], 'test.csv')\n",
    "    # Preprocess 클래스의 make_set_as_df 메서드를 사용하여 테스트 데이터를 데이터프레임으로 변환합니다. 이때, is_train=False로 설정하여 학습과 달리 요약(summary)이 포함되지 않은 테스트 데이터를 처리합니다.\n",
    "    # test_data['fname']는 테스트 데이터의 고유 ID로, 각 샘플을 식별하는 데 사용됩니다.\n",
    "    # 테스트 데이터를 데이터프레임으로 변환합니다. is_train=False로 설정하여 테스트 데이터셋을 로드합니다.\n",
    "    test_data = preprocessor.make_set_as_df(test_file_path, is_train=False)\n",
    "    test_id = test_data['fname']  # 테스트 데이터의 고유 ID (fname)을 가져옵니다.\n",
    "\n",
    "\n",
    "    # 데이터 확인:\n",
    "    # 테스트 데이터의 첫 번째 대화(dialogue) 샘플을 출력하여, 데이터가 올바르게 로드되었는지 확인합니다.\n",
    "    print('-' * 150)\n",
    "    print(f'test_data:\\n{test_data[\"dialogue\"][0]}')\n",
    "    print('-' * 150)\n",
    "    \n",
    "\n",
    "    ### 인코더 및 디코더 입력 생성:\n",
    "    # Preprocess 클래스의 make_input 메서드를 사용하여 인코더 입력과 디코더 입력을 생성합니다. \n",
    "    # 테스트 데이터의 경우, 디코더 입력은 시작 토큰(bos_token)만 포함합니다.\n",
    "    # 테스트 데이터에 대해 인코더 입력과 디코더 입력을 생성합니다. is_test=True로 설정하여 디코더 입력에만 시작 토큰을 넣습니다.\n",
    "    encoder_input_test, decoder_input_test = preprocessor.make_input(test_data, is_test=True)\n",
    "    print('-' * 10, 'Load data complete', '-' * 10,)\n",
    "\n",
    "\n",
    "    ### 토큰화:\n",
    "    # tokenizer를 사용하여 인코더 입력과 디코더 입력을 토큰화합니다. \n",
    "    # 이 과정에서 패딩, 특수 토큰 추가, 최대 길이 제한 등을 설정하여 텐서 형태로 반환합니다.    \n",
    "    # 테스트 데이터의 인코더 입력을 토크나이저로 토큰화합니다.\n",
    "    test_tokenized_encoder_inputs = tokenizer(\n",
    "        encoder_input_test, return_tensors=\"pt\", padding=True,\n",
    "        add_special_tokens=True, truncation=True,\n",
    "        max_length=config['tokenizer']['encoder_max_len'], return_token_type_ids=False,\n",
    "    )\n",
    "    # 테스트 데이터의 디코더 입력을 토크나이저로 토큰화합니다.\n",
    "    test_tokenized_decoder_inputs = tokenizer(\n",
    "        decoder_input_test, return_tensors=\"pt\", padding=True,\n",
    "        add_special_tokens=True, truncation=True,\n",
    "        max_length=config['tokenizer']['decoder_max_len'], return_token_type_ids=False,\n",
    "    )\n",
    "\n",
    "\n",
    "    ### 테스트 데이터셋 준비:\n",
    "    # DatasetForInference 클래스를 사용하여 토큰화된 입력 데이터와 테스트 ID를 포함한 데이터셋 객체를 생성합니다.\n",
    "    # 이 데이터셋 객체는 모델이 예측을 수행하는 데 사용됩니다.\n",
    "    test_encoder_inputs_dataset = DatasetForInference(test_tokenized_encoder_inputs, test_id, len(encoder_input_test))\n",
    "    print('-' * 10, 'Make dataset complete', '-' * 10,)\n",
    "\n",
    "\n",
    "    ### 결과 반환:\n",
    "    # 원본 테스트 데이터(test_data)와 모델에 입력될 토큰화된 데이터셋(test_encoder_inputs_dataset)을 반환합니다. \n",
    "    return test_data, test_encoder_inputs_dataset\n",
    "\n",
    "\n",
    "    # 이 함수는 테스트 데이터를 전처리하여 모델에 적합한 입력 데이터로 준비합니다. \n",
    "    # 이를 통해 학습된 모델이 테스트 데이터에 대한 예측을 정확하게 수행할 수 있도록 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######  테스트 데이터셋을 준비합니다.  ##########################################################################################\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "test_data:\n",
      "#Person1#: 더슨 씨, 받아쓰기 좀 해주세요. \n",
      "#Person2#: 네, 실장님...\n",
      "#Person1#: 이것은 오늘 오후까지 모든 직원에게 내부 메모로 전달되어야 합니다. 준비되셨나요?\n",
      "#Person2#: 네, 실장님. 시작하셔도 됩니다.\n",
      "#Person1#: 모든 직원들에게 주의하라... 즉시 효력을 발휘하여, 모든 사무실 통신은 이메일 통신과 공식 메모로 제한됩니다. 근무 시간 동안 직원들이 즉시 메시지 프로그램을 사용하는 것은 엄격히 금지됩니다.\n",
      "#Person2#: 실장님, 이것은 내부 통신에만 적용되는 건가요? 아니면 외부 통신에도 제한이 되는 건가요?\n",
      "#Person1#: 이것은 모든 통신에 적용되어야 합니다, 이 사무실 내의 직원들 사이뿐만 아니라 외부 통신에도 마찬가지입니다.\n",
      "#Person2#: 하지만 실장님, 많은 직원들이 고객과 소통하기 위해 즉시 메시지를 사용하고 있습니다.\n",
      "#Person1#: 그들은 그들의 의사소통 방법을 바꾸어야만 합니다. 이 사무실에서 누구도 즉시 메시지를 사용하지 않기를 원합니다. 너무 많은 시간을 낭비하게 됩니다! 이제, 메모를 계속해주세요. 우리가 어디까지 했나요?\n",
      "#Person2#: 이것은 내부와 외부 통신에 적용됩니다.\n",
      "#Person1#: 그렇습니다. 즉시 메시지를 계속 사용하는 어떤 직원이라도 먼저 경고를 받고 직무 정지에 처해질 것입니다. 두 번째 위반 시에는 직원은 해고에 처해질 것입니다. 이 새로운 정책에 대한 어떤 질문이라도 부서장에게 직접 문의하면 됩니다.\n",
      "#Person2#: 그게 다신가요?\n",
      "#Person1#: 네. 이 메모를 오후 4시 전에 모든 직원에게 타이핑하여 배포해 주세요.\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "---------- Load data complete ----------\n",
      "---------- Make dataset complete ----------\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n#######  테스트 데이터셋을 준비합니다.  ##########################################################################################\")\n",
    "# 테스트 데이터셋을 준비합니다.\n",
    "test_data, test_encoder_inputs_dataset = prepare_test_dataset(config, preprocessor, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 데이터 로더 생성 / 테스트 데이터 추론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######  데이터 로더를 생성합니다. 배치 크기는 설정 파일에서 가져옵니다.  ##########################################################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:49<00:00,  2.51it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n#######  데이터 로더를 생성합니다. 배치 크기는 설정 파일에서 가져옵니다.  ##########################################################\")\n",
    "# 데이터 로더를 생성합니다. 배치 크기는 설정 파일에서 가져옵니다.\n",
    "dataloader = DataLoader(test_encoder_inputs_dataset, batch_size=config['inference']['batch_size'])\n",
    "\n",
    "summary = []  # 요약문을 저장할 리스트\n",
    "text_ids = []  # 텍스트 ID를 저장할 리스트\n",
    "\n",
    "with torch.no_grad():  # 추론 중에는 기울기 계산을 비활성화하여 메모리를 절약합니다.\n",
    "    for item in tqdm(dataloader):  # 데이터 로더를 통해 배치 단위로 데이터에 접근합니다.\n",
    "        text_ids.extend(item['ID'])  # 현재 배치의 텍스트 ID를 저장합니다.\n",
    "        generated_ids = generate_model.generate(\n",
    "            input_ids=item['input_ids'].to(device),  # 인코더 입력을 디바이스로 이동시켜 모델에 전달합니다.\n",
    "            no_repeat_ngram_size=config['inference']['no_repeat_ngram_size'],  # 반복 n-gram 방지 설정\n",
    "            early_stopping=config['inference']['early_stopping'],  # 조기 종료 설정\n",
    "            max_length=config['inference']['generate_max_length'],  # 생성할 텍스트의 최대 길이\n",
    "            num_beams=config['inference']['num_beams'],  # 빔 서치(beam search)의 빔 수 설정\n",
    "        )\n",
    "        for ids in generated_ids:  # 생성된 요약문 ID를 디코딩하여 텍스트로 변환합니다.\n",
    "            result = tokenizer.decode(ids, skip_special_tokens=False)  # 특수 토큰을 건너뛰고 디코딩합니다.\n",
    "            summary.append(result)  # 생성된 요약문을 리스트에 추가합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) 최종 결과/요약문 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######  스페셜 토큰을 제거하여 최종 요약문을 전처리합니다.  ######################################################################\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n#######  스페셜 토큰을 제거하여 최종 요약문을 전처리합니다.  ######################################################################\")\n",
    "# 스페셜 토큰을 제거하여 최종 요약문을 전처리합니다.\n",
    "remove_tokens = config['inference']['remove_tokens']\n",
    "preprocessed_summary = summary.copy()\n",
    "for token in remove_tokens:\n",
    "    preprocessed_summary = [sentence.replace(token, \" \") for sentence in preprocessed_summary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(preprocessed_summary)):\n",
    "    tmp = preprocessed_summary[i]\n",
    "    left_trimmed = tmp.lstrip()  # 왼쪽 공백 제거\n",
    "    right_trimmed = left_trimmed.rstrip()  # 오른쪽 공백 제거\n",
    "    tmp = right_trimmed\n",
    "    tmp = tmp.replace('# ', '#')\n",
    "    preprocessed_summary[i] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['더슨 씨는 #Person1#에게 이메일 통신과 공식 메모로 제한되는 내부 메모에 대해 설명합니다. 실장님은 이것이 내부와 외부 통신에 적용된다고 말합니다. #Person1#은 즉시 메시지를 계속 사용하는 직원이라도 먼저 경고받고 직무 정지에 처해질 것이라고 합니다.',\n",
       " '#Person1#은 #Person2#에게 교통 체증에 걸렸다고 말한다. #Person2#는 #Person1#이 대중교통을 이용하기 시작하면 더 좋을 것이라고 생각한다. #Person1#은 출근할 때 운전하는 것을 그만두고 자전거를 타는 것을 제안한다.',\n",
       " '#Person1#은 케이트에게 마샤와 히어로가 이혼을 신청했다고 말한다. 케이트는 마샤가 양육권을 가지게 될 것이라고 믿지만, #Person1#은 그것이 믿기지 않는다.',\n",
       " '브라이언은 #Person1#에게 자신의 생일을 축하하고, #Person1#은 브라이언의 목걸이가 드레스와 잘 어울리길 바란다고 말한다.',\n",
       " '#Person2#는 #Person1#에게 올림픽 공원이 크다고 말합니다. #Person1#는 #Person2#에게 올림픽 스타디움에 대해 설명하고, #Person2#는 외국인 방문객을 위한 표지판을 많이 설치했다고 말합니다.',\n",
       " '#Person1#은 #Person2#에게 사업 계획서를 작성하고 투자자를 모집할 것이라고 말한다. #Person2#는 #Person1#에게 사업 아이디어를 두 페이지 이내로 강조하는 요약문과 회사에 대해 설명해야 한다고 말한다. #Person1#은 투자자들에게 가장 중요한 정보는 재무 분석이라고 말한다.',\n",
       " '#Person2#는 #Person1#에게 긁고 어지럽고 힘이 없다고 말한다. #Person1#는 #Person2#가 두드러기나 알레르기일 수도 있다고 말한다. #Person2#는 그것이 생물학적 위험 요소라고 생각한다.',\n",
       " '#Person2#는 세탁 서비스 비용을 청구하지 않았습니다. #Person1#은 세탁 서비스를 이용하지 않았으므로, #Person2#는 실수로 청구서를 수정할 것입니다.',\n",
       " '스티븐은 #Person1#에게 비서와의 불륜에 대해 불평한다. #Person1#은 스티븐에게 최선을 다해 그녀가 이혼을 재고하도록 설득할 것이다.',\n",
       " '#Person1#은 #Person2#에게 건전한 성격을 가진 남자 또는 여자로 눈에 띄는 사람들에 대해 이야기합니다. #Person2#는 에이브러햄 링컨을 존경하고 #Person1#은 그를 직접 만나고 싶어합니다.',\n",
       " '#Person1#은 허베이로 여행을 가려고 합니다. #Person2#는 중국 북부에서 심각한 모래폭풍이 일어나고 있다고 #Person1#에게 말합니다. #Person1#은 이것이 이 지역에 사는 사람들에게 영향을 미친다고 말합니다.',\n",
       " '프란시스는 #Person1#의 생일 파티에 왔습니다. #Person1#은 프란시스를 환영하고 리모컨 자동차를 #Person1#에게 주었습니다.',\n",
       " '스티븐이 토니에게 큰 실수를 했다고 말한다. 토니는 열심히 공부해야 한다고 생각한다.',\n",
       " '톰은 아홉시 10분 전에 가야 하고, #Person1#은 기차를 타야 한다.',\n",
       " '#Person1#은 #Person2#에게 삶을 어떻게 조정해야 할지 조언을 구합니다. #Person2#는 #Person1#에게 충분한 수면을 취하고 아침 운동을 하라고 제안합니다.',\n",
       " '#Person2#는 #Person1#에게 루오지아의 파티에 가고 싶다고 말합니다. #Person1#는 #Person2#에게 그녀의 트위터에서 뉴스를 봤다고 말하고, #Person2#는 루오지아의 집에서 와인잔 한 쌍과 그녀의 결혼을 축하하는 카드를 가져갈 것이라고 말합니다.',\n",
       " '#Person1#은 줄기를 당기고 뒷면을 벗기는 것을 제안한다. #Person2#는 이것이 잔인하다고 생각한다.',\n",
       " '#Person1#은 마이크와 #Person2#의 누나에 대해 이야기한다. 마이크는 #Person2#에게 자만심이 크다고 말한다.',\n",
       " '#Person2#는 머리가 아파서 #Person1#에게 전화를 합니다. #Person1#은 #Person2#에게 부모님에게 전화하도록 요청합니다.',\n",
       " '#Person2#는 #Person1#에게 카메라와 MP3 플레이어가 있는 새로운 휴대폰을 사달라고 요청합니다.',\n",
       " '주디가 프랭크에게 새로운 일자리를 얻었다고 말한다. 프랭크는 우체국에서 일하게 될 것이며, 주디의 가족들도 건강보험을 이용할 수 있다고 말한다.',\n",
       " '#Person2#는 #Person1#에게 컴퓨터 프로그램을 작성할 수 있고, 비서 기술에 능숙하며, 사무 기술에 대한 특별한 교육을 받았다고 말합니다.',\n",
       " '#Person1#은 #Person2#가 미디엄 레어로 스테이크를 주문하는 것을 도와줍니다. #Person2#는 만족스럽지 않다면 다른 것을 가져다 줄 것입니다.',\n",
       " '톰이 #Person1#에게 소설이 노벨상을 받았다고 말한다. #Person1#은 축하하러 갈 것이다.',\n",
       " '#Person2#는 #Person1#에게 #Person2#가 이 일을 할 수 있다고 생각하게 된 이유와 이전 직장에서 #Person2#가 담당했던 업무에 대해 설명합니다.',\n",
       " '#Person2#는 #Person1#에게 보통 맥주 두 병만 마신다고 말하고, 보통 밤새도록 10잔을 마신다고 말한다. #Person2#는 내일 밤에 램 바 앤 그릴에서 피처맥주 특별 할인을 한다고 말한다. #Person1#는 #Person2#에게 칵테일을 추천한다.',\n",
       " '#Person1#은 메에게 피크닉 준비를 도와달라고 요청한다. 메이는 토스트, 치킨 윙, 과일 샐러드, 그리고 피크닉 담요를 거실로 가져다 줄 것이다.',\n",
       " '뮤리엘 더글라스씨는 제임스에게 수잔 김이 곧 도착할 것이라고 말합니다. 제임스는 스키를 타는 것을 좋아하지 않지만, 수잔 김의 아내는 하와이처럼 따뜻한 곳에서 휴가를 보내는 것을 선호한다고 말합니다. 뮤리엘은 제임스에게 그의 동료인 수잔 김을 소개합니다.',\n",
       " '#Person1#은 유니버설 은행에 왔습니다. #Person2#는 #Person1#에게 카드를 슬롯에 넣으라고 요청합니다. #Person1#은 #Person2#에게 인출하고 싶은 금액을 입력하라고 요청합니다. #Person2#는 거절합니다.',\n",
       " '#Person2#는 #Person1#에게 #Person2#가 많은 친구들과 함께하는 것을 좋아하는 외향적인 사람이라고 말합니다.',\n",
       " '폴리 씨는 #Person1#에게 탄산음료를 사달라고 요청하지만, #Person1#은 상사가 그 가게에 있기 때문에 거절합니다. #Person1#은 #Person2#에게 직접 가보라고 제안합니다.',\n",
       " '프란시스는 모니카에게 재무 보고서를 언제 작업할 수 있는지 물어봅니다. 모니카는 금요일 오후에 가능하다고 말합니다.',\n",
       " '#Person1#과 #Person2#는 다음 주에 있을 면접을 준비하기 위해 워크숍을 진행하고 있습니다. #Person1#은 면접에서 도움이 될 것들을 배우고 있습니다. #Person2#는 면접관들이 항상 답변에 솔직해야 한다고 생각합니다.',\n",
       " '#Person1#은 컷에게 처음부터 다시 시작하자고 제안한다. #Person1#은 제이슨과 로라의 반응이 분노와 슬픔을 동시에 느끼는 것이 아니라고 생각한다. #Person2#는 다른 방법을 시도해 볼 것이다.',\n",
       " '#Person1#은 토드 부인을 방문하고 싶어한다. 토드 부인은 #Person1#에게 정원에 있는 당신을 보고 인사하러 가기로 결정했다고 말한다.',\n",
       " '빌은 #Person1#에게 이번 주에 매일 밤 10시까지 일했기 때문에 피곤하다고 말합니다. #Person1#은 #Person2#에게 오늘 퇴근 시간에 집에 가서 쉬라고 말합니다.',\n",
       " '클레오와 사이먼은 핵무기 확산 반대 시위에 대해 이야기하고 있다. 클레오는 평화로운 시위를 원하지만 사이먼은 물리학 시험을 공부할 계획이다. 사이먼은 클레오에게 평화로운 시위에 가자고 제안하지만 클레오는 거절한다.',\n",
       " '#Person1#은 #Person2#에게 누군가가 카드를 보여준다면 자동으로 그가 괜찮다고 생각하게 된다고 말합니다.',\n",
       " '#Person1#은 매기에게 역사 과목 노트를 빌려달라고 요청한다. 매기는 보통 식당에서 그것을 복습하러 가므로, 마크는 매일 슈퍼마켓에서 일하기 때문에 노트를 빌릴 필요가 없다. 그들은 오늘 도서관에서 시작할 것이다.',\n",
       " '#Person1#은 터너 교수님에게 고급 지질학 과목을 다시 강의할 수 있는지 물어봅니다. 터너 교수는 #Person1#에게 충분히 어렵지 않았다고 말합니다. #Person1#은 그에게 등록을 허락해 줄 것입니다.',\n",
       " '#Person1#은 #Person2#에게 펜던트가 부러졌다고 말합니다. #Person2#는 #Person1#에게 영수증을 가지고 매장으로 오라고 요청합니다.',\n",
       " '#Person1#와 #Person2#는 경기를 보고 있다. #Person1#은 #Person2#에게 옷을 바꾸고 싶어한다. #Person2#는 #Person1#에게 살즈베리 씰즈를 욕하지 말라고 제안한다.',\n",
       " '#Person2#는 #Person1#에게 미디어 쪽에서 일하는 것이 재미있을 것 같다고 말한다. #Person1#은 #Person2#에게 잡지를 위해 글을 쓰는 것을 제안한다.',\n",
       " '#Person1#과 #Person2#는 지루한 연설을 하고 있다. #Person1#은 #Person2#에게 집중해서 듣고 노트를 해야 한다고 말한다. #Person2#는 십자말풀이를 싫어하고 #Person1#은 십자말풀이 싫어한다.',\n",
       " '사라는 #Person1#에게 도심에서 멀리 떨어진 곳에 더 저렴한 집을 사는 것을 제안한다. #Person1#은 사라의 시누이와 남편이 방금 그런 식으로 집을 샀다고 말한다. 사라는 이사할 생각이 없다.',\n",
       " '#Person1#은 런던의 정보 담당관인 마크 리치를 소개합니다. 마크 리치는 브리튼 비즈니스 센터의 정보 담당관으로 해외에서 온 방문객들을 대상으로 관광 정보 서비스를 제공합니다. 그는 런던에서 하루 여행을 원한다면 어디로 가고 어떻게 가야 하는지 제안할 수 있습니다. 마크 리치는 영국에 오는 사람들에게 좋은 제안이 가능한 많이 보는 것이라고 말합니다.',\n",
       " '린팡과 루시는 다음 수업에 대해 이야기하고 있다. 린팡은 영어를 가장 좋아하고, 루시는 수학을 가장 좋아한다. 둘 다 자신이 좋아하는 과목이다.',\n",
       " '제임스 부인이 토마스 부인에게 마당에 낙엽을 치우고 오스카를 산책시켜 달라고 요청합니다. 토마스 부인은 일요일에 오스카를 도와줄 예정입니다. 제임스 부인은 자전거를 구입한 후에도 계속 일할 수 있는지 묻습니다.',\n",
       " '#Person1#과 #Person2#는 봄이 왔지만 밤에는 여전히 춥다고 생각합니다. #Person2#는 #Person1#에게 에어컨을 켜고 더운 바람을 불어넣으라고 제안합니다.',\n",
       " '컷은 마이크에게 그녀가 더 이상 보고 싶지 않다고 말하지만 제이슨과 로라는 3년 동안 함께했기 때문에 그의 반응이 화와 슬픔의 혼합이 될 수 없다고 말한다. 마이크는 #Person1#에게 자신의 방법으로 해보라고 조언한다.',\n",
       " '#Person1#은 택시를 타고 프렌드십 호텔로 갑니다. #Person2#는 #Person1#에게 거스름돈을 지불하라고 요청합니다.',\n",
       " '#Person1#은 배가 고파서 다른 버스를 타야 합니다. #Person2#는 #Person1#에게 환승 요금을 알려줍니다.',\n",
       " '#Person2#는 #Person1#에게 #Person2#의 회사가 시내에 있다고 말합니다. #Person1#는 #Person2#에게 회사 이름과 직원 식당에 대해 알려줍니다.',\n",
       " '#Person1#은 #Person2#에게 루루와 빅이 헤어졌다고 말한다. #Person2#는 그들이 사귀고 있다는 것을 몰랐다.',\n",
       " '데이브 톰슨이 샐리에게 전화를 걸어 짐이 아직 돌아오지 않았다고 말한다. 샐리는 톰슨에게 나중에 다시 전화할 것이라고 말한다.',\n",
       " '#Person1#은 시청으로 가는 방법을 #Person2#에게 알려줍니다. #Person2#는 #Person1#에게 두 블록만 더 걸어가면 왼쪽에 시청이 보일 것이라고 말합니다.',\n",
       " '#Person1#은 #Person2#에게 여권을 반납한 사람이 있는지 묻습니다. #Person2#는 #Person1#에게 분실물 신고서를 작성하라고 요청합니다. #Person1#은 분실물을 찾지 못하면 새 여권을 발급받을 수 있다고 말합니다.',\n",
       " '나다니엘과 레아는 콜린스 선생님에게 전화하려고 했지만 거절당했습니다. 레아는 오늘 오후 리우 씨와 통화할 수 있다고 말합니다.',\n",
       " '#Person2#는 #Person1#에게 #Person2#가 딕 이외의 남자와 결혼하지 않겠다고 말했을 때 무서웠다고 말합니다. #Person2#는 사라는 그녀를 사랑하지 않겠다고 했습니다.',\n",
       " '#Person1#과 #Person2#는 파티에 대해 이야기하고 있다. #Person1#은 파티에 있는 제니스를 보고 흥분하고 있다. #Person2#는 #Person1#에게 조금 더 맥주 마시고 분위기에 취해보라고 제안한다.',\n",
       " '#Person1#은 잭에게 이번 학기 수업에 대해 묻습니다. 잭은 정치학 수업과 비즈니스 커뮤니케이션 수업을 좋아합니다.',\n",
       " '#Person1#과 #Person2#는 어젯밤의 날씨에 대해 이야기하고 있습니다. #Person1#은 베이징의 가을이 아름답다고 생각하지만 겨울에는 너무 춥다고 생각합니다. #Person2#는 #Person1#에게 감기 걸리지 않게 옷을 더 챙겨 입으라고 조언합니다.',\n",
       " '#Person1#은 #Person2#에게 공포 영화를 보자고 제안하지만 #Person2#는 추리 영화를 좋아하지 않습니다. #Person1#은 로맨스는 지루하다고 생각합니다. #Person2#는 전쟁 영화를 보러 갈 것입니다.',\n",
       " '#Person1#은 아담에게 학교를 구경시켜달라고 요청한다. 아담은 #Person1#에게 도서관과 새 교실 건물에 대해 설명한다. 그들은 언젠가 여기 학교에 입학할 수 있기를 희망한다.',\n",
       " '#Person1#은 #Person2#에게 아기가 곧 태어날 것이라는 소식을 전한다.',\n",
       " '#Person1#과 #Person2#는 존이 그녀와 일곱 번이나 만나는 것에 대해 이야기합니다.',\n",
       " '#Person1#은 시스템 업그레이드를 고려하고 있습니다. #Person2#는 #Person1#에게 소프트웨어에 페인팅 프로그램을 추가하는 것을 제안합니다. #Person1#은 그것이 보너스가 될 것이라고 생각합니다.',\n",
       " '홀리데이 인인 #Person2#는 중국에서 왔습니다. #Person2#는 중국 사람 같진 않지만 멕시코 사람입니다. #Person1#는 #Person2#가 너무 젊어 보인다고 생각합니다. #Person2#는 스페인어로 이야기하기로 결정했습니다.',\n",
       " '#Person1#은 너무 많이 먹어서 체중이 증가했다고 #Person2#에게 말합니다. #Person2#는 식단 조절을 제안하지만 #Person1#은 운동 수업에 가자고 제안합니다.',\n",
       " '제임스는 8인 룸을 원하지만 현재 룸은 이용하지 못한다. #Person1#은 제임스를 직원에게 안내할 것이다.',\n",
       " '#Person2#는 #Person1#에게 공장이 70년대 초에 설립되었으며 곧 30주년을 맞이할 것이라고 말합니다. #Person1#는 #Person2#의 도움으로 조립 라인을 시작합니다.',\n",
       " '레베카는 #Person1#에게 대학 졸업 후 첫 직장이 요크 헤럴드였다고 말합니다. 그녀는 지역 뉴스 기자 자리를 제안받았지만 2006년에 떠나기로 결정했습니다.',\n",
       " '#Person1#은 그룹 발표를 위해 문구점에 가야 합니다. #Person2#는 #Person1#에게 먼저 쇼핑 목록을 만들어 보라고 제안합니다. #Person1#은 마커, 색연필, 형광펜, 브러시, 압정, 수정테이프, 클립을 가져올 것입니다.',\n",
       " '#Person1#은 메리에게 인터넷에서 일자리를 찾아보라고 제안한다. 메리는 #Person1#에게 어떻게 일자리를 찾을 수 있는지 알려준다.',\n",
       " '#Person1#은 #Person2#에게 쇼핑 예산을 세우고 한 달에 $ 300만을 저축해야 한다고 말한다. #Person2#는 #Person1#의 계획을 칭찬한다.',\n",
       " '헨리는 제인에게 수잔을 보려고 병원에 가는 중이라고 말한다. 제인은 13번 버스를 탈 것이다. 헨리는 13번을 탈 것이다.',\n",
       " '#Person2#는 내년 판매 예측에 대해 #Person1#에게 이야기하고 싶어합니다. #Person1#은 #Person2#에게 스프레드시트를 어떻게 사용하는지 설명해달라고 요청합니다. #Person2#는 동의합니다.',\n",
       " \"#Person1#은 #Person2#에게 '뉴욕의 친구'라는 투어 가이드 서비스를 추천한다. #Person2#는 설문조사를 통해 정보를 제공하고, #Person1#의 예산에 맞는 완벽한 여행을 계획할 수 있다.\",\n",
       " '#Person1#은 #Person2#에게 회사가 어린이용 장난감을 생산하고 영업 부서에서 일하고 싶다고 말합니다. #Person2#는 월급으로 2,000 위안을 받고 그 외에 수당과 보너스를 받고 있습니다. #Person2#는 복리후생 혜택과 보험에 대해 #Person1#에게 설명합니다.',\n",
       " '#Person1#은 계약서에 서명하러 왔습니다. #Person2#는 #Person1#에게 수정이 필요한 부분이 있는지 확인해달라고 요청합니다. #Person1#은 동의합니다.',\n",
       " '#Person2#는 #Person1#에게 나이아가라 폭포로 가는 길에 차량 사고가 났다고 말합니다. #Person1#은 #Person2#에게 구급차와 경찰을 불러달라고 요청합니다.',\n",
       " '#Person2#는 #Person1#에게 학교 클리닉으로 가는 방법을 알려줍니다.',\n",
       " '#Person2#는 #Person1#의 방이 너무 시끄러워서 #Person2#의 방을 바꾸고 싶어합니다. #Person1#은 #Person2#에게 내일 저녁에 조용한 스위트룸에서 편안하게 머무르라고 제안합니다.',\n",
       " '#Person2#는 #Person1#에게 베이징으로 가는 비행이 편안했지만, 지금은 조금 피곤하다고 말합니다. #Person1#은 #Person2#를 위한 연회에 초대하고 저녁 식사는 오후 6시에 국제 호텔에서 열립니다.',\n",
       " '#Person2#는 길을 잃었습니다. #Person1#은 #Person2#에게 길을 알려줍니다. #Person2#는 리우 이창으로 가려고 합니다.',\n",
       " '#Person1#은 컴퓨터가 잘 작동하지 않아 리 씨에게 도움을 청하려고 한다. #Person2#는 수리공에게 전화하는 것을 제안한다.',\n",
       " '#Person1#은 #Person2#에게 어머니의 생신 선물로 목걸이와 금 시계를 추천합니다. #Person2#는 그것을 좋아합니다.',\n",
       " '로스 씨는 피셔 씨에게 전화를 걸어 피셔 씨가 뉴질랜드에 지사를 곧 열 예정이라고 알리고 피셔 씨를 초대합니다. 피셔 씨는 발표 시간과 날짜를 확인하고 커피, 간식 뷔페에 대한 비용을 청구할 것입니다. 피셔 씨는 또한 토요일이어서 가족들과 함께 보내고 싶어합니다. 그들은 100장의 초대장을 보냈고, 피셔 씨는 200명이 올 수도 있다고 말합니다. 그들은 대략적인 숫자를 기준으로 예산을 세울 것입니다.',\n",
       " '#Person1#과 #Person2#는 러시아와 캐나다의 주요 차이점에 대해 이야기하고 있습니다. #Person1#은 캐나다 사람들이 더 빠르게 움직인다고 생각합니다. #Person2#는 캐나다인들이 미국인들보다 더 여유롭다고 생각하며, 캐나다 정부가 문제를 해결하고 새로운 기술을 사용하여 사업을 잘 운영하는 데 훌륭한 일을 한다고 생각합니다.',\n",
       " '#Person1#과 #Person2#는 올해 휴가를 어디로 갈지에 대해 이야기하고 있습니다. #Person1#은 카리브해로 가기로 결정하면 행운을 빌어야 한다고 생각합니다.',\n",
       " '#Person1#은 소풍 갈 때 가져갈 과일을 #Person2#에게 요청합니다. #Person2#는 포도를 가져갈 것입니다.',\n",
       " '#Person2#는 #Person1#에게 소형차 렌트 비용을 알려주고 운전 면허증을 보여줍니다.',\n",
       " '#Person2#는 #Person1#에게 엘리베이터 안에서 미소를 짓는 것이 다른 이용자들을 불편하게 만들 수 있다고 말합니다. #Person1#는 #Person2#에게 무표정을 유지하는 사람들이 질렸다고 말합니다.',\n",
       " '#Person1#과 #Person2#는 판매 리뷰에 대해 이야기하고 있습니다. #Person2#는 #Person1#에게 올해 성장률이 놀랍다고 말합니다. #Person1#은 #Person2#의 재정적 어려움이 완전히 끝났으면 좋겠다고 희망합니다.',\n",
       " '#Person2#의 가방이 도난당했다. #Person1#은 #Person2#에게 택시를 타고 집에 가서 엄마에게 돈을 달라고 할 것이다. #Person2#는 #Person1#에게 50달러를 빌려줄 것이다.',\n",
       " '스티븐은 #Person1#에게 베이징에서 팁을 어떻게 처리하는지 설명하고, 중국에서는 서비스 요금을 청구서에 추가하지만 팁을 주지 않는다고 말한다.',\n",
       " '빌은 룸메이트 브레인 로커에 대해 #Person1#에게 이야기한다.',\n",
       " '#Person1#은 #Person2#에게 청구서를 지불하고 싶어합니다. #Person2#는 #Person1#에게 호텔 서비스와 식사비를 포함하여 660달러를 청구합니다.',\n",
       " '수잔이 캐롤에게 전화를 걸어 캐롤의 파티가 오늘 밤인지 내일 밤인지 물어봅니다. 캐롤은 수잔에게 내일 밤 8시 30분에 질의 집에서 파티가 있다고 말합니다.',\n",
       " '#Person1#은 #Person2#에게 커먼 헬스장에서 일하거나 시내의 영화관에서 일했다고 말합니다. #Person2#는 #Person1#이 자신을 다른 사람과 혼동했다고 생각합니다.',\n",
       " '#Person1#은 #Person2#에게 트럼프가 다시 우리 대통령이 된다면 상상도 할 수 없다고 말합니다. #Person2#는 바이든에게 투표할 것입니다.',\n",
       " '#Person1#은 ATM을 사용해야 합니다. #Person2#는 #Person1#에게 카드를 기계에 넣고, 원하는 옵션을 클릭하는 방법을 알려줍니다.',\n",
       " '수잔이 존에게 사무 절차에 대한 메모의 복사본을 달라고 요청한다.',\n",
       " '#Person1#은 릴리에게 이번 주말에 소풍에 참여할 수 있는지 묻습니다. 릴리는 강가로 가서 둘러보고 저녁 식사를 할 것입니다.',\n",
       " '#Person2#는 #Person1#에게 중국 테이블에서 식사하는 것이 #Person2#의 고향과 어느 정도로 다른지 설명합니다. #Person1#은 #Person2#에게 중국 식사 자리에서 적절하게 행동해야 한다고 말합니다. #Person2#는 동의하지만 한 쌍의 젓가락에는 긴 이야기가 있습니다.',\n",
       " '메리와 프랭크는 여가 시간에 어떤 영화를 보는지에 대해 이야기한다. 메리는 예술 영화를 좋아하고, 프랭크는 보통 무비 살롱에서 영화를 빌려 본다. 그들은 무비살롱의 회원이다.',\n",
       " '#Person1#은 #Person2#에게 녹색당에 가입하는 것을 생각해 본 적이 있다고 말합니다. #Person2#는 녹색당이 선거에서 이길 가능성이 없다고 생각합니다. #Person1#은 정치에 대해 그다지 알지 못하며, 미디어는 종종 정치 이슈를 보도합니다.',\n",
       " '#Person1#은 윌슨 씨에게 사과하고 샘플에 미치지 못하는 모든 상품을 교환하겠다고 약속했다.',\n",
       " '#Person2#는 #Person1#에게 강도가 은행 안에 있었다고 말합니다. #Person1#는 추가 질문을 위해 경찰서로 오라고 요청합니다.',\n",
       " '#Person1#은 배가 고프다. #Person2#는 #Person1#에게 일주일에 한 번씩 피자를 주문하라고 제안한다.',\n",
       " '캐롤은 새해 결심에 대해 #Person1#에게 말하고, 캐롤은 식습관을 바꾸기 위해 다이어트를 시작하기로 결정했다.',\n",
       " '카렌 후앙은 비교문학 287 수업에 등록하고 싶어합니다. #Person1#은 카렌에게 전화로 등록하려고 했지만 컴퓨터에서 안 된다고 말합니다. 카렌은 #Person1#에게 대학 컴퓨터 시스템이 비교문학 학생들을 위해 287 수업을 추가 자리를 남겨두었다고 알려줍니다. 카렌은 전화로 등록할 때 특별 코드를 요청합니다.',\n",
       " '#Person2#는 급해서 우산을 깜빡하고 왔습니다. #Person1#은 #Person2#에게 가든 호텔로 가는 길을 알려줍니다.',\n",
       " '#Person1#은 잭에게 새 차를 태워달라고 요청한다. 잭은 비싸다고 생각하지만 빠르게 달릴 수 있다고 생각한다.',\n",
       " '#Person2#는 #Person1#에게 큰 불이 밤 10시쯤 일어났다고 말합니다.',\n",
       " '#Person2#는 #Person1#에게 900위안의 서비스 요금을 지불하라고 요청합니다. #Person1#은 #Person2#의 남편이 신용카드를 가지고 있다고 말합니다. #Person2#는 영수증을 포장합니다.',\n",
       " '#Person1#은 #Person2#의 차를 세차하고 싶어합니다. #Person2#는 #Person1#에게 일반 세차 패키지를 추천하고, #Person1#은 그것을 선택합니다.',\n",
       " '해리는 #Person1#에게 올해 휴가를 어디에서 보낼지 정하지 못했다고 말한다. #Person1#은 바다를 통해 여행할 수도 있다고 제안한다. 해리는 동의한다.',\n",
       " '존슨이 #Person1#에게 기계를 어떻게 사용하는지 알려준다. #Person1#은 존슨에게 제대로 스트레칭하고 워밍업을 하라고 조언한다. 존슨은 기계를 사용하면서 워밍업을 하고 트레이닝 카드를 발급받는다. #Person2#는 존슨에게 기계를 사용하는 방법을 알려준다. 존슨은 처음에는 너무 밀어붙이지 않는 것이 중요하다고 생각한다. 존슨은 그만두는 것이 최선이라고 생각한다.',\n",
       " '#Person2#는 #Person1#에게 회사가 문을 닫아서 일자리를 잃었다고 말한다. #Person1#는 집세를 내기 위해 필요한 일이라면 무엇이든지 할 것이라고 말한다. #Person2#는 전기기사 프로그램에 지원하려고 한다.',\n",
       " '#Person1#은 #Person2#에게 강아지들에게 밥을 주고 목욕을 시켜달라고 요청합니다. #Person2#는 동의합니다.',\n",
       " '에이든은 집주인에게 200달러를 빚지고 있다. #Person1#은 그에게 돈을 빌리고 싶어한다. 에이든은 #Person1#에게 저녁 식사를 제안한다. #Person1#은 동의한다.',\n",
       " '#Person2#는 #Person1#에게 자선 단체에서의 경험이 #Person2#의 사고 방식에 직접적인 영향을 미쳤다고 말합니다.',\n",
       " '#Person2#는 #Person1#에게 대략 한 주 동안 결정을 내려달라고 요청합니다.',\n",
       " '#Person1#은 #Person2#에게 중국 와인을 추천하고 베이징 오리구이를 추천합니다. #Person2#는 시도해볼 것입니다.',\n",
       " '댄이 안젤라에게 전화를 걸어 안젤라의 친구가 다음 주에 결혼하는 것을 알려주려고 했다고 말한다. 안젤라는 메건을 데리러 갈 것이다.',\n",
       " '#Person2#는 #Person1#에게 전통 그리스 요구르트부터 시작해서 이탈리아 티라미수부터 시작할 것이라고 말한다. #Person1#은 #Person2#에게 바나나 튀김을 가져오라고 요청한다.',\n",
       " '스미스 씨는 #Person1#에게 자신이 예일 대학교를 다녔고, 경제학 학사 학위를 가지고 있으며, 지난 5년 동안 은행에서 일했다고 말했습니다.',\n",
       " '#Person1#은 #Person2#의 조카를 위해 좋은 것을 사고 싶어합니다. #Person2#는 #Person1#의 가게의 베스트셀러인 바비를 추천하고 #Person1#은 그것을 현금으로 결제합니다.',\n",
       " '#Person1#은 조던 스포츠 신발 한 켤레를 #Person2#에게 추천합니다. #Person2#는 이미 세일 중입니다.',\n",
       " '#Person2#는 과학 박물관에 가고 싶지만 길을 잃어버려서 티켓 기계를 어떻게 사용해야 할지 모른다. #Person1#은 #Person2#에게 4번 플랫폼에서 기차를 타라고 조언한다.',\n",
       " '사이먼은 #Person1#에게 은퇴 후의 기분과 단계적 은퇴 프로그램에 대해 이야기한다. #Person1#은 #Person2#에게 단계적 은퇴 프로그램이 어떻게 운영되는지 알려준다. #Person2#는 이것이 #Person1#의 시간을 직접 관리할 수 있다고 생각한다.',\n",
       " '로키는 #Person1#에게 애정이 많고 모든 요구를 충족시켜주는 여자를 좋아한다고 말한다. #Person1#은 결혼할 수 없을 것이라고 생각한다. 로키는 외향적이고, 사람들의 차이를 비판하지 않는 여자를 좋아한다. #Person1#은 TV 저녁식사와 러스티와 집에 갈 예정이다.',\n",
       " '#Person1#과 #Person2#는 어제 밤에 심한 폭풍에 대해 이야기하고 있습니다. #Person1#은 #Person2#에게 폭풍이 끔찍하다고 불평합니다. #Person2#는 봄이 왔으면 좋겠다고 희망합니다.',\n",
       " '#Person2#는 #Person1#에게 TV를 보는 것이 지루하다고 생각한다. #Person1#는 #Person2#가 더 많은 것들을 배울 수 있는 다양한 활동들이 있다고 생각한다. #Person2#는 함께 나눌 수 있는 것을 제안한다.',\n",
       " '#Person1#은 새로운 학교 생활에 적응하기 위해 내일 수업에 대해 벤에게 걱정하고 있다. 벤은 #Person1#에게 휴식 시간에 먹을 것을 사다 달라고 요청한다.',\n",
       " '애덤은 #Person1#에게 무릎 상태가 나아졌다고 말한다. #Person1#은 그에게 던지기 연습을 하라고 조언한다. 애덤은 내일 전체 훈련에 참여할 예정이다.',\n",
       " '#Person1#은 프린터가 고장나서 #Person2#에게 복사본을 출력해달라고 요청합니다. #Person2#는 거절합니다.',\n",
       " '#Person1#은 커튼을 걸고 싶어한다. #Person2#는 #Person1#의 커튼을 걸어줄 것이다.',\n",
       " '잭은 #Person1#에게 다다음 주말에 캠핑 여행을 계획하고 있다고 말한다.',\n",
       " '#Person1#은 #Person2#에게 산불을 끄기 위해 밤과 밤을 가리지 않고 일한 것에 대해 사과한다. #Person2#는 #Person1#에게 아기가 임신한 것 같다고 말한다. #Person1#은 아기가 정상적으로 보인다고 말한다.',\n",
       " '#Person1#은 #Person2#에게 딸이 스스로 결정을 내릴 수 있도록 격려하라고 조언한다. #Person2#는 동의한다.',\n",
       " '#Person1#은 #Person2#에게 직장을 잃게 되면 생계를 유지하지 못하게 될 것이라고 말한다. #Person2#는 #Person1#에게 어떻게든 돈을 투자하고 싶다고 말한다. #Person1#는 #Person2#가 돈을 절약하면서 살면 충분히 돈을 절약할 수 있다고 생각한다.',\n",
       " '#Person2#는 #Person1#에게 #Person2#의 친구가 에든버러 대학교의 박사과정 학생이며 올해 말에 졸업할 예정이라고 말한다. #Person1#는 #Person2#에게 그의 강연 주제를 물어본다. #Person2#는 동의한다.',\n",
       " '#Person1#은 #Person2#에게 존의 집에 가자고 제안하지만, #Person2#는 #Person1#이 아파서 거절한다.',\n",
       " '파버 씨는 요크 호텔에 3박 동안 더블룸 하나를 예약하고 싶어합니다. #Person1#은 파버 씨에게 더블룸 하나를 제공합니다.',\n",
       " '#Person1#은 웨스트 더비를 추천하지만, #Person2#는 혼자 살기 때문에 저렴한 단칸방이나 연립 주택을 원합니다. #Person1#은 존 고드프리에게 토요일에 그를 만날 수 있을 것이라고 말합니다.',\n",
       " '#Person2#는 #Person1#에게 경찰이 #Person2#에게 길을 물어보자고 제안했을 때 들어줬다면 처음부터 길을 잃지 않았을 것이라고 말한다. #Person1#는 #Person2#가 너무 늦었다고 생각한다.',\n",
       " '댄은 #Person1#에게 주문한 100대의 컴퓨터가 2일이나 지연되었다고 말합니다. #Person1#은 댄에게 스티브에게 전화해서 이 사항에 대해 말하라고 요청합니다.',\n",
       " '#Person1#은 #Person2#에게 워싱턴 포스트 복사본을 보여줍니다. #Person2#는 #Person1#에게 로스앤젤레스 타임즈가 1881년 12월에 처음 출판되었다고 말합니다.',\n",
       " '#Person1#은 #Person2#의 컴퓨터에 바이러스가 침투한 것 같아서 이메일을 보낼 수 없다. #Person2#는 #Person1#에게 압축해서 보내라고 제안한다.',\n",
       " '#Person1#은 주말에 #Person2#를 초대했습니다. #Person2#는 #Person1#에게 감사의 말을 전합니다.',\n",
       " '#Person1#은 햄버거, 중국 음식, 그리고 판다 익스프레스에서 중국 음식을 사올 예정입니다.',\n",
       " '톰이 메리에게 판매 직원 자리를 다른 사람에게 제공하기로 결정했다고 알려줍니다. 메리는 빠르게 배우는 사람이고 초과 근무를 할 의향이 있습니다.',\n",
       " '#Person1#이 #Person2#에게 열 번째 전화를 걸었습니다. #Person2#는 경찰에 신고할 것입니다.',\n",
       " '#Person2#는 음악과 예술 분야의 학사와 석사 학위를 가지고 있습니다. #Person2#는 #Person1#에게 음악 선생님이 되고 싶다고 말합니다. #Person2#는 음악이 사람의 감정과 성격에 영향을 준다고 생각하며, 클래식 음악을 듣는 것이 스트레스를 줄이는 데 도움이 된다고 말합니다. #Person1#는 #Person2#에게 추천을 해달라고 요청하고, #Person2#는 동의합니다.',\n",
       " '#Person2#는 #Person1#에게 오늘 집 아래에 사는 여자를 만났다고 말합니다. #Person1#는 #Person2#의 이웃과 사귀는 것은 편리하지만 헤어진다면 어떻게 할지에 대해 걱정하고 있습니다. #Person2#는 언젠가 그녀를 저녁 식사에 초대할 생각입니다.',\n",
       " '#Person1#과 #Person2#는 미렐라가 사무실에서 집처럼 편하게 지내는 것에 대해 이야기한다. 그들은 캘리포니아의 업무 분위기가 훨씬 더 편안하고 캐주얼하다고 생각한다.',\n",
       " '#Person1#은 #Person2#에게 자신만의 법률 사무소를 세우려고 합니다. #Person2#는 #Person1#에게 도와달라고 요청합니다.',\n",
       " '케이트는 피터에게 어젯밤에 온라인 게임을 했다고 말한다. 피터는 휴식이 필요하다고 생각한다.',\n",
       " '#Person1#과 #Person2#는 요즘 배와 보트가 예전처럼 교통수단으로 중요하지 않다고 생각합니다. 왜냐하면 사람들의 생활 속도가 빨라지고 있기 때문입니다.',\n",
       " '앤디는 패니에게 어젯밤 악몽을 꿨다고 말한다. 앤디는 악몽을 꾸고 나서 다시 잠들기가 무서웠다. 패니는 앤디에게 미시간 대학교에 들어가는 것에 대해 걱정하고 있다고 말한다.',\n",
       " '#Person1#과 #Person2#는 밴드를 시작할 예정이다. 그들은 바닐라 아이스의 노래를 연주할 예정이다.',\n",
       " '#Person1#은 #Person2#에게 오늘 밤 강변 보트 투어, 생어 극장, 보트 투어를 제안합니다. #Person2#는 동의합니다.',\n",
       " '#Person1#은 #Person2#의 신용카드를 사용하여 #Person2#의 사이즈에 맞는 물건을 구매했습니다.',\n",
       " '#Person1#은 블레이크 씨에게 포스터 씨가 훈련 매뉴얼을 보내줄 수 있는지 묻습니다. 블레이크 씨는 #Person1#에게 복사 중이라고 말합니다.',\n",
       " '데이비드는 #Person1#에게 이번 주 금요일에 롱아일랜드에서 출발하여 솔트레이크시티까지 4일 동안 운전을 해서 갈 계획이라고 말한다. 데이비드는 호텔에서 묵고 지나가는 길에 현지 음식을 즐길 예정이다.',\n",
       " '#Person1#은 #Person2#에게 파멜라의 비행기를 호출하고 파멜라를 돌봐달라고 요청한다. #Person2#는 파멜라가 #Person1#의 진정한 친구라고 생각한다. #Person1#은 파멜라를 만나면 파멜라에게 편지를 쓸 것이다.',\n",
       " '#Person2#는 #Person1#에게 이 도시에서 가장 붐비는 거리인 77번 거리를 알려줍니다. #Person1#는 12번 버스를 선택합니다.',\n",
       " '#Person1#은 #Person2#에게 무슬림들이 메카로 순례를 가는 이유를 설명한다. #Person2#는 무슬림들은 적어도 한 번은 하지를 가야 한다고 믿는다고 말한다. #Person1#은 사람들이 치유받으러 가는 루르드가 마법 같은 특별한 곳이라고 생각한다.',\n",
       " '#Person2#는 #Person1#에게 센트럴 백화점으로 가는 길을 알려줍니다. #Person1#은 #Person2#에게 국립은행이 어디에 있는지 알려줍니다.',\n",
       " '#Person2#는 #Person1#에게 다음 주에 런던에 가자고 제안한다. #Person1#는 기차를 선호하지만 #Person2#는 차를 좋아한다. #Person2#는 고속버스를 선호한다.',\n",
       " '캐서린과 톰은 점심에 대해 이야기하고 있다. 톰은 패스트푸드가 건강에 좋지 않다고 생각하지만, 캐서린은 그것이 미국인의 라이프 스타일이라고 생각한다.',\n",
       " '#Person2#는 #Person1#의 도움으로 바비큐 윙과 베이비 백 립을 주문했다.',\n",
       " '#Person1#은 #Person2#의 도움으로 더블 치즈버거, 용수철 감자튀김, 펩시콜라 사이즈를 주문했습니다.',\n",
       " '#Person1#은 #Person2#에게 머핀, 계란, 그리고 프라이한 계란을 만들어 달라고 요청한다. #Person2#는 #Person1#에게 반숙 계란에 대해 몇 번 말해야 한다고 말한다.',\n",
       " '#Person1#은 #Person2#를 그랜드 호텔로 데려갑니다.',\n",
       " '#Person1#은 아이들에게 미국 경찰이 어떻게 생겼는지 보여주고 싶어합니다. #Person2#는 동의합니다.',\n",
       " '#Person2#는 #Person1#에게 선셋 호텔에 전화하여 빈 방을 예약하도록 요청합니다.',\n",
       " '#Person1#은 #Person2#에게 미국에 공부하러 가기 위해 비자를 신청했다고 말합니다. #Person2#는 신청서가 매우 복잡하다고 생각합니다. #Person1#은 많은 사람이 비자 조건에 관한 규정을 어기기 때문이라고 생각합니다.',\n",
       " '앤은 #Person1#에게 지난 밤 즐거웠지만 다시 보고 싶지 않다고 말한다. #Person1#은 앤에게 내일 밤 술집에서 만날 수 있는지 물어본다.',\n",
       " '메리는 #Person1#에게 온라인 쇼핑이 편리하고 시간을 절약할 수 있다고 말합니다. #Person1#은 온라인 은행에 계좌를 개설해야 한다고 말합니다. 메리는 온라인으로 모든 물건을 찾을 수 있다고 생각합니다.',\n",
       " '#Person2#는 미국식 회계에 익숙하지 않습니다. #Person1#은 #Person2#에게 회계 과정에서 가장 기본적인 개념에 대해 설명합니다.',\n",
       " '제인은 피터에게 여름에 시안에 가자고 제안한다. 피터는 모니카는 하얼빈에 가고 피터는 북부 지방에 사시는 부모님이 있으면 좋을 것 같다고 말한다. 피터는 돈이 많으면 휴가를 어디로 가고 싶냐고 묻는다. 제인은 남서부 지방으로 가고 피터는 남서부 지방에 가고 싶어한다.',\n",
       " '#Person1#은 로맨스 영화를 대여하고 싶어한다. #Person2#는 #Person1#에게 무료 비디오 대여를 추천하고, #Person1#은 #Person2#에게 회원 양식을 작성하라고 요청한다. #Person1#은 또한 연체료는 비디오 대여료와 동일하므로 기간 안에 반납해야 한다.',\n",
       " '#Person1#은 미스터 리를 찾고 있습니다. #Person2#는 #Person1#의 책상 위에 서명을 할 것입니다.',\n",
       " '#Person2#는 #Person1#에게 공항이 폐쇄되어 내일 아침까지 비행기가 뜨지 못한다고 말합니다. #Person1#은 불을 켜놓지 않으면 잠을 잘 수 없다고 말합니다. #Person2#는 나쁜 습관이 있다고 말합니다.',\n",
       " '#Person1#은 #Person2#를 대접하고 싶어합니다. #Person2#는 #Person1#에게 식당을 찾아달라고 요청합니다.',\n",
       " '#Person2#는 요즘 잠이 잘 오지 않는다. #Person1#는 #Person2#에게 긴장을 푸는 요가 수업을 듣고 이완 요법을 배우라고 조언한다. #Person2#는 #Person1#에게 음악을 듣는 것을 제안한다. #Person1#는 동의한다.',\n",
       " '#Person1#은 인테리어가 구식이라고 생각하지만, #Person2#는 아름답고 깨끗한 부엌을 원한다.',\n",
       " '스털링은 월터에게 우드 교수님이 뛰어난 과학자라고 생각한다.',\n",
       " '#Person1#은 #Person2#에게 한 번 더 검사 결과를 받아보라고 요청합니다. #Person2#는 #Person1#에게 오후에 다시 찾아오라고 요청합니다.',\n",
       " '마틴이 엘리자 선생님에게 다가오는 시험 준비를 위해 열심히 공부하고 있다고 말한다. 램 선생님은 마틴의 문제를 해결하는 데 도움을 주었고, 학생 복지 클럽에 대해 불평하지 않는다.',\n",
       " '#Person1#은 #Person2#에게 소포를 특급으로 한국에 특급으로 보내달라고 요청합니다. #Person2#는 패키지 우편으로 보내겠다고 합니다.',\n",
       " '린다는 #Person1#에게 호텔 방을 뒤졌는데도 휴대폰이 보이지 않는다고 말한다. #Person1#은 린다에게 여동생이 애들 어떻게 지내는지 알려주려고 #Person1#의 휴대폰을 잃어버렸다고 알려준다.',\n",
       " '#Person2#는 #Person1#에게 로스앤젤레스 출신이며 이틀 더 있을 예정이라고 말합니다. #Person1#는 #Person2#에게 음료를 더 가져다 줄 것입니다.',\n",
       " '#Person1#은 여름방학 동안 공부에서 벗어나고 싶어한다. #Person2#는 #Person1#에게 졸업 후에 갈 멋진 장소를 생각해 보라고 조언한다.',\n",
       " '메리는 #Person1#에게 오늘 아침에 우유를 배달하러 갔는데 12번지의 정원 문이 잠겨 있었다고 말한다. 메리는 울타리를 뛰어넘어 문 쪽으로 갔지만 거대한 개가 자신을 향해 달려왔다. 메리는 개에게 화를 낸다.',\n",
       " '#Person2#는 #Person1#에게 #Person2#의 회사가 예전보다 더 열정적이라고 말합니다. #Person1#는 #Person2#에게 새로운 기술을 배울 수 있게 도와주고 여행할 기회가 그리 많지 않다고 말합니다.',\n",
       " '#Person1#은 당좌예금 계좌를 개설하고 싶어합니다. #Person2#는 #Person1#에게 이자를 지급하지 않는다고 알려줍니다. #Person1#은 돈을 인출하는 방법을 알려줍니다.',\n",
       " '#Person1#은 중국 요리를 먹어보고 싶어합니다. #Person2#는 #Person1#에게 광동 요리와 강소 요리를 추천하고 #Person1#은 광동 요리를 추천합니다. #Person1#은 베이징 오리 구이로 유명한 전취덕 식당을 #Person2#에게 추천합니다.',\n",
       " '#Person1#은 배송이 느려서 걱정하고 있습니다. #Person2#는 #Person1#에게 양모가 빨리 도착하기를 바랍니다. #Person1#은 #Person2#에게 계약서를 작성하라고 요청합니다.',\n",
       " '#Person2#가 #Person1#의 독서등을 깨뜨렸습니다. #Person1#은 #Person2#에게 양식을 작성하라고 요청합니다.',\n",
       " '#Person1#은 2006 회계 연도 마케팅 계획에 대해 #Person2#에게 설명합니다. #Person2#는 해외 시장에서 배송을 두 배로 늘리는 목표를 설정했습니다. #Person1#은 #Person2#가 목표를 달성했는지 평가하기 위해 고려해야 할 세 가지 사항에 대해 설명합니다.',\n",
       " '#Person1#은 #Person2#에게 휴가 동안 유럽을 여행할 계획이라고 말합니다. #Person2#는 파리에서 니스로 기차를 타고 가고, 밀라노에서 박물관을 방문할 예정입니다.',\n",
       " '지미 폭스씨는 중형 차량 예약을 했습니다. #Person1#은 예약을 유지하는 방법을 모르기 때문에 예약을 하지 않았다고 말합니다. 폭스씨는 컴팩트나 SUV를 원한다면 예약을 할 수 있다고 말합니다. #Person1#은 보험을 들어줄 것입니다.',\n",
       " '#Person1#은 아빠가 공부방에서 일하고 있다고 엄마에게 말한다. 엄마는 #Person1#에게 5분 동안 소리를 내지 말라고 경고한다.',\n",
       " '앤은 존스 씨에게 이번 주에 회의 일정에 대해 알려준다.',\n",
       " '#Person2#는 #Person1#에게 세계 지도를 보고 있다고 말한다. #Person2#는 지리적 특성만 보여준다고 말한다. #Person1#는 #Person2#에게 인터넷에서 더 많은 정보를 찾아보라고 조언한다.',\n",
       " '#Person2#는 금연 방을 예약했지만 방에서 담배 냄새가 너무 심해서 방을 바꾸고 싶어합니다. #Person1#은 바로 금연 방을 준비해 줄 것입니다.',\n",
       " '#Person1#은 빌에게 젖은 페인트에 손대지 말라고 경고한다. 빌은 존 샘슨처럼 무심코 행동하지 않을 것이다.',\n",
       " '벤이 #Person1#에게 가야 할 시간이 되었다고 말합니다. #Person1#은 엘라의 전화번호를 얻기 위해 다음 주에 #Person1#을 전화할 것입니다.',\n",
       " '짐은 빌에게 멋진 딕이 이탈리아에서 돌아온 후 아팠다고 말한다. 빌은 윌리엄 박사가 가능한 한 빨리 일할 수 있다고 했다고 말한다.',\n",
       " '#Person1#은 #Person2#의 회사가 #Person1#의 생명력을 빨아들이고 있다고 생각한다. #Person2#는 #Person1#에게 최저임금으로 일하고 있고 야근에 대한 추가 급여를 주지 않는다고 말한다. #Person1#은 다른 일자리를 찾지 못할까봐 두려워한다.',\n",
       " '리사는 #Person1#에게 마크가 두 달 동안 다른 사람을 만나고 있었다는 것을 알게 되었다고 말한다. #Person1#은 그에게 진실을 말하거나 그녀와의 관계를 끝내지 않으면 이혼하겠다고 말했다고 리사는 말한다.',\n",
       " '#Person1#은 #Person2#에게 적색 육류를 줄이고 식단을 정리하라고 조언했습니다.',\n",
       " '#Person1#은 #Person2#에게 가야 한다고 말하지만 #Person2#는 가야 한다고 말한다. #Person2#는 #Person1#에게 함께 인생을 함께 해달라고 요청한다.',\n",
       " '#Person2#는 #Person1#에게 도서관 이용 방법을 알려줍니다. #Person2#는 한 번에 두 권의 책을 대출하고 기한 내에 책을 반납하지 못하면 벌금을 내야 합니다.',\n",
       " '#Person1#과 #Person2#는 날씨가 좋은 날에 점심을 먹기로 결정했습니다. 그들은 날씨가 계속 좋다면 해변에 가기로 결정했습니다.',\n",
       " '#Person2#는 #Person1#에게 카메라를 돌려주고 슬라이드나 그림 엽서를 사는 것을 도와줍니다.',\n",
       " '#Person2#는 #Person1#에게 대출 신청에 대한 정보를 제공합니다. #Person1#는 #Person2#의 신용 점수가 매우 낮다고 생각합니다. #Person2#는 은행이 대출을 승인하기 위해 개인 정보, 과거 대출, 자산 및 신용 점수와 같은 기타 관련 정보를 평가해야 한다고 말합니다.',\n",
       " '모니카는 #Person1#에게 발표가 성공적이었다고 말한다. #Person1#은 모니카를 칭찬하고 모니카의 노력이 보상받았다고 생각한다.',\n",
       " '#Person1#은 톰에게 러닝하러 가자고 제안한다. 톰은 아침에 달리는 것을 좋아하므로, #Person1#은 조깅을 위해 #Person1#을 찾아갈 것이다.',\n",
       " '#Person2#는 #Person1#에게 일본 레스토랑의 메뉴와 영업 시간에 대해 알려줍니다.',\n",
       " '#Person1#은 심슨 씨에게 목요일에 점심을 먹자고 제안합니다. #Person2#는 동의합니다.',\n",
       " '#Person1#은 #Person2#에게 데이트를 레스토랑에 데려가고, 그래마시 타번이 좋은 평가를 받았다고 말합니다. 그들은 테이블을 예약하려고 합니다.',\n",
       " '#Person1#은 #Person2#에게 오리구이와 소고기 한 그릇에 50달러를 지불하라고 요청합니다. #Person2#는 팁을 받지 않습니다.',\n",
       " '#Person2#는 #Person1#에게 #Person2#의 나라에서 축구를 좋아하고, 일부 사람들은 농구를 좋아하며, 일부 사람들은 테니스를 좋아한다고 말합니다. #Person2#는 익스트림 스포츠는 소수의 사람들만을 위한 것이라고 생각합니다. #Person1#는 정기적으로 골프를 치는 사람들을 많이 알고 있습니다.',\n",
       " '#Person1#은 헬싱키, 핀란드로 가는 비행기 예약을 하고 싶어합니다. #Person2#는 #Person1#에게 가능한 비행기를 확인해보라고 요청하고, #Person1#은 #Person2#에게 채식주의자 식사를 요청합니다.',\n",
       " '#Person1#과 #Person2#는 내일 플로리다로 가서 할머니를 방문할 예정입니다.',\n",
       " '#Person1#은 호주에 가고 싶어합니다. #Person2#는 대장벽 산호초를 보고 싶어합니다.',\n",
       " '#Person1#은 로라에게 몸매 관리를 위해 헬스장에 가는 중이라고 말한다. 로라는 #Person1#에게 그녀가 걷는 것을 좋아한다고 말한다. 그녀는 친구들과 정기적으로 테니스를 치고, 가끔 혼자 수영하거나 자전거를 타는 것을 좋아한다.',\n",
       " '#Person1#은 #Person2#에게 시카고에서 태어났고, 고등학교를 졸업했으며, 유럽에 갔다고 말합니다. #Person2#는 #Person1#에게 뮌헨에 머물렀다고 말합니다.',\n",
       " '#Person1#은 큰 쇼핑 센터에서 #Person2#의 새로운 정장을 샀습니다. #Person2#는 그것이 좋은 거래라고 생각하지 않습니다.',\n",
       " '앤은 마사 자전거 클럽의 회장인 로빈에게 산호수 자전거 투어에 대해 설명합니다. 로빈은 사이클리스트들이 출발할 때 도로를 막는 문제를 피하기 위해 분산 출발을 도입했습니다.',\n",
       " '#Person1#은 #Person2#에게 애퍼티프, 컴파리, 시그니처 음료, 싱거, 그리고 무알콜 칵테일을 주문합니다.',\n",
       " '#Person1#은 그레고리와 점심을 먹으러 가자고 제안하지만 그레고리는 번지 점프를 하고 싶어한다. #Person1#은 #Person2#에게 스스로 해보라고 제안하지만 #Person2#는 번지 점프가 너무 무섭다고 말한다.',\n",
       " '#Person1#은 제2차 세계 대전에 대한 정보를 찾고 싶어합니다. #Person2#는 #Person1#에게 노르망디 상륙작전에 대한 웹사이트를 추천합니다. #Person1#은 관심이 생기면 도서관에 가보라고 제안합니다.',\n",
       " '#Person2#는 #Person1#에게 기계를 사용하기 위해 어떻게 훈련해야 하는지 알려준다. #Person1#는 근육질이 되고 싶지 않다. #Person2#는 처음에는 무게를 조금 줄이는 것을 제안한다. #Person1#는 그것이 좋다고 생각한다.',\n",
       " '#Person2#는 #Person1#에게 캠퍼스 내에 주차할 공간이 필요하다고 말합니다. #Person1#는 캠퍼스 서쪽으로 가면 찾을 수 있다고 말합니다.',\n",
       " '#Person1#은 수잔에게 급여에 대해 몇 가지 질문을 한다. 수잔은 FICA, SUI/SDI 세금, 사회보장과 메디케어, 그리고 건강보험 계획에 대한 공제에 대해 묻는다. 수잔은 #Person1#에게 연방 공제 아래에는 주 공제가 있다고 말한다. #Person1#은 이것이 수잔에게 도움이 될 것이라고 생각한다.',\n",
       " '#Person1#은 간식을 사고 싶어한다. #Person2#는 #Person1#에게 돈을 슬롯에 넣으라고 조언한다. #Person1#은 동의한다.',\n",
       " '#Person1#은 #Person2#에게 태국에 여자친구를 만나러 가려고 한다고 말한다. #Person2#는 #Person1#에게 인터넷이 상호작용을 훨씬 빠르게 만들어 사람들이 훨씬 더 빨리 서로를 알게 한다고 말한다.',\n",
       " '제임스는 케이트에게 새 가구를 사고 장식을 다시 하려고 돈을 모았다고 말한다. 케이트는 새 카펫을 좋아한다. 제임스는 카펫이 예전 카펫 같다고 생각하고, 케이트는 그것을 좋아한다.',\n",
       " '빌은 #Person1#에게 아침 식사 전에 조깅을 하고, 점심시간에 30분 동안 체육관에서 운동하며, 일주일에 세 번 수영이나 라켓볼이나 핸드볼을 한다고 말한다.',\n",
       " '마르켓은 #Person1#에게 올해 졸업하려면 과학 과목을 수강해야 한다고 말합니다. #Person1#은 생물학, 화학, 지도학, 물리학을 수강해야 합니다.',\n",
       " '#Person2#는 #Person1#에게 #Person2#의 프로젝트와 더욱 친환경적인 삶을 어떻게 이끌어갈 것인지에 대해 이야기한다. #Person2#는 모든 학교 종이가 재활용되고 많은 친구들이 종이 양면을 사용하고 있다고 생각한다. #Person2#는 대부분의 작업을 전자 제출해야 한다고 생각한다. #Person1#는 학교 카페테리아를 추천한다.',\n",
       " '#Person1#은 토니에게 크리스마스가 다가오고 있다고 말한다. 토니는 쇼핑 센터의 장난감 부서에서 일하고 있으며, 어제는 늦잠을 자서 늦잠을 잤다. 그들은 크리스마스 전에 아이들을 위한 선물을 파는 일에 대해 불평한다.',\n",
       " '빌은 #Person1#에게 생일 파티를 위해 케이크를 만들어 달라고 요청하지만, #Person1#은 빌이 알레르기 반응을 피하기 위해 특정 음식을 먹지 말라고 했기 때문에 케이크를 먹을 수 없다고 말한다. #Person1#은 #Person2#에게 샐러드를 가져다 달라고 요청하지만 빌은 거절한다.',\n",
       " '#Person2#는 #Person1#의 여름 옷을 사고 싶어합니다. #Person1#은 #Person2#에게 티셔츠와 어울리는 치마와 바지를 추천합니다. #Person2#는 동의합니다.',\n",
       " '#Person1#은 72시간 이내에 재확인하지 않는 모든 예약은 취소된다고 #Person2#에게 말합니다. #Person2#는 다음 것을 원합니다.',\n",
       " '#Person2#는 #Person1#에게 부활절이 재회하기 좋은 날이라고 말한다. #Person2#는 뷔페 파티를 열고 아이들은 집에서 기다리라고 말한다.',\n",
       " '#Person1#과 #Person2#는 토니의 나쁜 습관에 대해 이야기하고 있다. #Person1#은 토니에게 보상함으로써 토니가 협력하도록 동기를 부여하는 것을 제안한다.',\n",
       " '#Person1#은 줄리와 알렉스가 결혼할 것이라고 #Person2#에게 말한다. #Person2#는 알렉스가 남자들만의 파티를 준비하고 있다고 말한다. #Person1#은 #Person2#의 친구 웬디가 결혼할 때 그녀의 이별 파티에 가는 것을 신경 쓰지 않을 것이다.',\n",
       " '#Person1#은 투표용지를 어디서 받아야 할지 모른다. #Person2#는 #Person1#에게 투표 부스로 가서 투표하라고 조언한다.',\n",
       " '#Person1#은 #Person2#에게 회사가 곧 인력을 줄일 것이라고 말한다. #Person2#는 앤디와 리사가 연애를 하고 있다는 소문을 들었다고 말한다. #Person1#은 마이클이 해고될까 봐 걱정하고 있다. #Person2#는 걱정한다.',\n",
       " '#Person2#는 중국 스타일 이혼이라는 중국 TV 시리즈를 보고 있다. #Person1#는 그것이 #Person2#의 생활과 매우 가깝다고 생각한다. #Person2#는 이혼이 빠르게 끝나는 경향이 있다고 생각한다. #Person1#는 동의하지 않는다.',\n",
       " '주디는 #Person1#에게 교통비로 4,000 RMB를 지출하고 호텔에 많은 돈을 지불해야 한다고 말한다. #Person1#은 저렴한 호텔을 선호한다.',\n",
       " '#Person1#은 어제 메리가 제로드와 결혼했다고 말한다. #Person2#는 놀라워한다.',\n",
       " '#Person2#는 지갑을 잃어버렸습니다. #Person1#은 #Person2#에게 50달러를 빌리고 #Person2#는 #Person1#에게 책을 사러 가고 주유소에 갈 것이라고 말합니다.',\n",
       " '#Person1#은 #Person2#에게 머피 뮤직과 U-튠즈가 합병한다는 소문을 들었다고 말합니다. #Person2#는 그 소문을 믿지 않지만, #Person1#은 그것이 의심스럽다고 생각합니다.',\n",
       " '톰은 조의 새로운 비서가 거만한 것 같다고 생각하지만, 조는 그것이 도움이 된다고 생각한다.',\n",
       " '사라는 회의에 대해 #Person1#에게 이야기한다. #Person1#은 사무실의 업무 흐름을 효율적으로 만들기 위한 아이디어를 생각해 내기 위해 회의를 주재한 밥에 대해 이야기한다. #Person2#는 회의가 효율적이지 않았다고 생각한다. #Person1#은 다음 회의에서 사라를 격려한다.',\n",
       " '#Person2#는 #Person1#에게 무슬림들이 메카로 순례를 가는 이유에 대해 설명한다. 무슬림들은 능력이 있는 모든 남성이 한 번은 하즈를 가야 한다고 믿는다. #Person1#은 프랑스 루르드에 치유된 사람들의 이야기가 많다고 생각한다.',\n",
       " '테드는 마이크에게 제니에게 고백할 용기가 없다고 말한다. 마이크는 그녀에게 어떻게 느끼는지 알려야 한다고 제안하지만, 테드는 거절한다.',\n",
       " '#Person2#는 #Person1#에게 화가 날 때 음악을 듣고 운동을 하는 것이 화를 풀기 좋은 방법이라고 말합니다.',\n",
       " '#Person1#은 #Person2#에게 뉴질랜드에 있는 가족에 대해 묻습니다. #Person2#는 #Person1#의 삼촌 빌, 그의 아내와 두 딸, 그리고 사라의 계모인 사라의 계모에 대해 이야기합니다.',\n",
       " '스튜어트 씨는 #Person1#에게 도시 마라톤에서 우승했다고 말합니다. #Person1#과 #Person2#모두 흥분하고 기뻐합니다.',\n",
       " '#Person1#은 #Person2#에게 아이들과 아내를 위한 선물을 고르고 있습니다. #Person2#는 #Person1#에게 비싼 신발과 향수를 추천하지만, #Person1#은 그것이 가품이라고 믿지 않습니다. 결국, #Person2#는 DKNY 브랜드를 선택합니다.',\n",
       " '홍은 로밍 요금을 감당할 수 없어서 #Person1#에게 현지 SIM 카드를 사용하라고 제안한다.',\n",
       " '브라운 씨는 #Person1#에게 한 달에 1,800 위안을 받고 있으며, 처음에는 2,500 위안을 지급할 것이라고 말합니다.',\n",
       " '#Person1#과 #Person2#는 배리와 폴에 대해 이야기하고 있다. 배리는 항상 새로운 사람들과 이야기하고 있다. #Person2#는 #Person1#에게 그들이 낯을 가리지 않는다고 말한다.',\n",
       " '#Person2#는 파사디나나 아카디아에 관심이 있습니다. #Person1#는 전망이 있는 집을 선호합니다. #Person2#는 #Person1#에게 딱 맞는 집을 찾아줄 것입니다.',\n",
       " '#Person1#은 #Person2#에게 샘플들을 남겨두고 가라고 요청합니다. #Person2#는 동의합니다.',\n",
       " '#Person2#는 #Person1#에게 Sons에 가려면 버스를 두 번 타고, 261번 버스를 타고 Sons까지 남은 길을 가야 한다고 말합니다.',\n",
       " '이 씨가 워드 여사님을 집까지 태워다 주고 우산을 들어줍니다.',\n",
       " '#Person1#은 #Person2#에게 자켓과 셔츠를 벗고 침대에 누워달라고 요청합니다. #Person2#는 #Person1#에게 어깨 엑스레이를 찍는 것을 제안합니다.',\n",
       " '#Person2#는 #Person1#의 카드를 잃어버렸습니다. #Person1#은 #Person2#에게 카드를 어디에 두었는지 물어보고, 셸리는 #Person1#에게 호텔 로비 바에서 카드를 잃어버렸다고 말합니다. #Person2#는 셸리에게 도움을 줄 것입니다.',\n",
       " '#Person1#은 #Person2#에게 전화기를 뽑아놓으라고 요청한다. #Person2#는 저녁에 대해 꿈도 꾸지 않을 것이다.',\n",
       " '#Person2#는 #Person1#에게 중국 음식, 서양 음식, 그리고 소고기 세트를 제공합니다. #Person1#는 #Person2#에게 도움을 요청합니다.',\n",
       " '#Person1#과 #Person2#는 식기를 가져오고 줄을 서기로 결정했습니다. #Person1#은 크림 케이크를 좋아하지 않습니다.',\n",
       " '#Person1#은 루시에게 스탠리가 노래하는 것을 듣게 해달라고 요청한다. 루시는 동의한다.',\n",
       " '#Person2#는 #Person1#에게 홈 비디오 플레이어가 영화관을 대체할 것이라고 말합니다. #Person1#는 영화 상영의 개념을 혁신해야 한다고 생각합니다.',\n",
       " '#Person1#은 #Person2#에게 갈색 드레스를 추천합니다. #Person2#는 가벼운 것을 원합니다. #Person1#은 노란색이나 초록색이 잘 어울릴 것이라고 생각합니다.',\n",
       " '#Person1#은 아빠가 매주 토요일마다 돈을 잊어버려서 13달러의 빚을 지게 되었습니다. #Person1#은 아빠의 비밀 돈통을 빌리고 일부를 가난한 사람들에게 주고 나머지는 책을 사려고 합니다.',\n",
       " '#Person1#은 #Person2#에게 마이크의 생일 파티에 초대받았다고 말한다. #Person2#는 일 끝나고 차로 그의 파티에 갈 것이다.',\n",
       " '#Person1#은 #Person2#에게 소포와 우표를 우등 우편으로 보내고 싶어합니다. #Person2#는 #Person1#에게 우표 창구에서 현금으로 주문할 수 있다고 말합니다.',\n",
       " '#Person2#는 #Person1#에게 중국의 지진이 세계에서 가장 심각한 자연 재해 중 하나라고 말합니다. #Person2#는 중국이 많은 파괴적인 지진에 시달렸다고 말합니다. #Person1#는 중국이 자연 재해에 익숙하지만, 원춘 지진을 마주할 때는 여전히 약해지고 있다고 말합니다.',\n",
       " '#Person2#는 #Person1#에게 장마가 곧 끝날 것이라고 말합니다. #Person1#는 습하고 추운 날씨를 견디기 힘들다고 생각하지만, #Person2#는 따뜻한 날씨를 좋아합니다.',\n",
       " '#Person1#과 #Person2#는 배구 경기 후에 올림픽 기념품 가게에 가서 무엇을 살지에 대해 이야기합니다. #Person1#은 인형 다섯 개를 세트로 팔고 크기에 따라 가격이 매겨져 있다고 말합니다. #Person2#는 할인을 받을 수 있을 것 같지 않다고 생각합니다.',\n",
       " '#Person1#은 #Person2#의 도움으로 16인치 알루미늄 팬을 구입합니다. #Person2#는 #Person1#에게 그것을 보여줍니다. #Person1#은 그것이 가볍다고 생각합니다.',\n",
       " '#Person1#과 #Person2#는 퇴근 후에 만나기로 결정했습니다.',\n",
       " '베커 씨는 #Person1#에게 워싱턴의 공무원들이 자신들의 일을 즐기는 이유를 설명합니다. #Person1#은 급여와 근로 조건이 매력적이라고 생각합니다. 베커 씨는 급여와 근로조건에 대해 이야기합니다.',\n",
       " '피트는 헨리 존슨을 만나고 싶어합니다. 헨리는 브루클린에서 태어나고 자랐고 브루클린의 브루클린으로 이사왔습니다. 피트와 헨리는 오늘 저녁의 계획을 정하고 있습니다. 그들은 노래방에서 저녁 식사를 할 예정입니다.',\n",
       " '줄리는 지난 일요일에 동창들 몇 명이랑 좋은 식당에서 점심을 먹었습니다. 존스 선생님은 줄리가 식중독에 걸렸다고 생각했지만 지금은 좀 나아졌다고 말했습니다. 줄리는 존스 선생님에게 수업을 다 따라갈 수 있다고 말했습니다.',\n",
       " '마이크는 오늘 저녁에 친구들이 몇 명 올 예정이다. 엄마는 마이크에게 음료를 준비하고 커피를 내릴 것이라고 말한다. 마이크는 설탕과 과일을 사올 것이다.',\n",
       " '#Person1#과 #Person2#는 베를린으로의 긴 버스 여행에 대해 이야기하고 있다. #Person1#은 #Person2#가 너무 피곤해 보인다고 생각한다. #Person2#는 다음에 #Person1#을 만나러 올 때 비행기를 탈 것이다.',\n",
       " '#Person2#는 #Person1#에게 이 일을 맡게 되면 주말에도 일해야 한다고 말한다. #Person2#는 외국어를 꽤 잘하고, 일본어를 대략 1년 동안 공부해 왔다고 말한다.',\n",
       " '에이미는 어제 저녁에 제니랑 빌이 자신을 소풍에 초대해서 집에 늦게 왔다. 에이미는 어제 베이하이 공원에서 점심을 먹고, 강가를 따라 산책하고, 친구들을 사귀었다. 지미는 에이미에게 책을 돌려주려고 했다고 말한다. 그들은 30분 후에 아래층 카페에서 만날 것이다.',\n",
       " '피터는 #Person1#에게 정원에 물을 줘야 한다고 말한다. #Person1#은 비가 오기 때문에 정원에 물을 줄 필요가 없다고 말한다. #Person2#는 동의한다.',\n",
       " '#Person1#은 #Person2#에게 가능한 한 빨리 비상 회의를 소집하라고 요청합니다. #Person2#는 #Person1#에게 켄이 돌아올 때까지 일정을 잡으라고 요청합니다.',\n",
       " '#Person1#은 #Person2#에게 편지에 붙일 우표를 구입하는 데 도움을 줍니다. #Person2#는 10달러 지폐를 입금합니다.',\n",
       " '스미스 씨는 #Person1#에게 감염을 치료하기 위해 항생제와 크림을 처방하고 약국에서 할인을 받을 수 있다고 말합니다.',\n",
       " '#Person2#는 #Person1#에게 탕나라에서 시작된 삼색 도자기, 자수, 바틱, 옥 조각을 추천합니다. #Person1#는 종이절편 자수와 바틱을 추천합니다.',\n",
       " '#Person1#과 #Person2#는 에펠탑에 대해 이야기하고 있습니다.',\n",
       " '브라이언은 #Person1#에게 대학에서 영어를 배웠고 캘리포니아에 가본 적이 없다고 말한다.',\n",
       " '#Person1#은 #Person2#에게 한 시간 전에 여기에 있어야 했다고 말한다. #Person2#는 #Person1#에게 그를 걱정하지 말라고 말한다.',\n",
       " '#Person1#은 #Person2#에게 큰 문제에 휘말릴 것이라고 경고한다. #Person2#는 #Person1#에게 일의 긍정적인 면을 보지 않는다고 말한다.',\n",
       " '#Person1#과 #Person2#는 존이 일주일에 일곱 번 그녀와 데이트를 한다는 사실에 동의한다.',\n",
       " '#Person1#과 #Person2#는 런던에 대해 이야기하고 있습니다. #Person1#은 웨스트민스터 대성당, 보아디케아의 동상, 그리고 유명한 성과 감옥, 그리고 밀랍 인형 박물관에 대해 #Person2#에게 이야기합니다.',\n",
       " '다니엘은 #Person1#에게 이번 학기에 몇 가지 새로운 과목을 추가했다고 말합니다. 다니엘은 과학을 가장 좋아하고, #Person1#은 과학에 관심이 있습니다. 다니엘은 이 과목을 통해 #Person1#의 부모님이 기뻐할 것이라고 생각합니다.',\n",
       " '#Person1#과 #Person2#는 베이비 샤워를 즐기고 있다. #Person1#은 베티가 준 선물과 칼라가 준 놀이펜과 침대를 #Person2#에게 선물로 준다.',\n",
       " '#Person1#은 #Person2#에게 중국에 관광하러 가자고 제안하지만, #Person2#는 너무 바빠서 거절한다.',\n",
       " '팀과 카렌이 서로 인사를 나눈다. 그들은 곧 다시 만날 예정이다.',\n",
       " '#Person1#은 어제 밤에 마이클을 만나러 갔다. #Person2#는 #Person1#에게 마이클이 오토바이를 좋아하지만 자전거를 사고 싶어한다고 말한다.',\n",
       " '#Person2#는 #Person1#에게 #Person2#가 중국어 외에도 영어와 프랑스어를 할 수 있다고 말합니다. #Person2#는 영어를 충분히 잘해서 사무 업무를 수행하는 데 충분하다고 생각합니다.',\n",
       " '낸시가 타일러네에 전화를 걸어 앤디에게 메시지를 전해달라고 요청한다.',\n",
       " '#Person1#은 뉴욕행 비행기가 취소되었다고 #Person2#에게 알립니다. #Person2#는 #Person1#에게 내일 비행기를 예약해 달라고 요청합니다. #Person1#은 50% 할인을 받을 것입니다.',\n",
       " '#Person1#과 #Person2#는 점심에 버거 퀸에 가서 치즈버거와 프렌치 프라이를 먹기로 결정한다. #Person1#은 너무 많은 튀김 음식은 #Person2#를 살찌게 한다고 생각하지만, #Person2#는 그것을 참는다.',\n",
       " '#Person1#은 #Person2#에게 초과 수하물 요금을 내야 한다고 말합니다.',\n",
       " '#Person1#은 목이 말라서 소다를 마시고 싶어합니다. #Person2#는 소다가 목마름을 해소하지 못한다고 생각합니다. #Person1#은 물을 마시고 싶어합니다.',\n",
       " '#Person2#는 #Person1#에게 그린 씨가 량 씨에게 사과의 말을 전하기 위해 오늘 해외로 가야 한다고 말한다. #Person1#은 #Person2#에게 다시 약속을 잡을 것이라고 말한다.',\n",
       " '#Person1#은 30사이즈의 유광 가죽 신발 한 켤레를 원합니다. #Person2#는 그것을 선택합니다.',\n",
       " '벤자민은 #Person1#에게 프로젝트 보고서를 어떻게 써야 할지 모른다고 말한다. #Person1#은 보통 연구 보고서만 쓴다고 말한다. 벤자민은 마이크로소프트 워드를 어떻게 사용하는지 모른다. #Person2#는 벤자민에게 조언을 구한다.',\n",
       " '#Person2#는 #Person1#의 도움으로 해산물 피자를 주문합니다. #Person1#은 #Person2#에게 특별 메뉴를 추천하고 #Person2#의 주소를 알려줍니다.',\n",
       " '#Person1#은 #Person2#에게 새우 칵테일, 토마토 수프, 계란 수프, 미네랄 워터 한 잔을 주문하는 데 도움을 줍니다.',\n",
       " '#Person2#는 #Person1#에게 #Person2#의 집이 멋진 거실, 넓은 식당, 그리고 편안한 침실 3개라고 말합니다. 부엌은 꽤 현대적입니다.',\n",
       " '#Person2#는 #Person1#에게 개인용 스테레오의 앞면에 큰 긁힘이 있다고 말합니다. #Person1#는 그것이 고객님의 책임이라고 생각하지만, #Person2#는 그것에 만족하지 않습니다. #Person1#는 영수증을 보여줍니다. #Person2#는 영수증에 문제가 있다고 생각합니다.',\n",
       " '#Person1#은 내년 9월에 기숙사 방을 원한다. #Person2#는 #Person1#에게 캠퍼스 밖에서 살 것을 제안한다. #Person1#은 감당할 수 있을 것 같지만 #Person2#는 더 생각해봐야 한다고 생각한다.',\n",
       " '#Person1#은 중고 서점을 둘러보는 것을 즐긴다. #Person2#는 오래된 어린이 이야기 책을 #Person1#에게 보여줍니다. #Person1#은 그 책을 사서 도서관에서 비슷한 이름을 찾아보려고 합니다.',\n",
       " '#Person2#는 #Person1#에게 베이징 레스토랑의 창가 옆 테이블을 보여줍니다. #Person1#은 #Person2#에게 음식을 20분 후에 준비해 달라고 요청합니다.',\n",
       " '#Person1#은 편지를 받고 싶어합니다. #Person2#는 #Person1#에게 공인 우편이나 등기 우편으로 보내는 것을 권장합니다. #Person1#은 시계의 가치만큼 보험을 들어야 합니다.',\n",
       " '#Person1#은 샐리에게 톰이 보낸 편지를 읽어달라고 요청한다. 샐리는 존에게 편지를 보낸다. #Person1#은 톰이 1월에 도시에 방문할 예정이라고 말한다.',\n",
       " '#Person1#은 좌절감을 느끼고 있습니다. #Person2#는 #Person1#이 스스로 컴퓨터를 구입할 날을 기대하고 있다고 생각합니다.',\n",
       " '#Person1#과 #Person2#는 여름이 끝난 후의 날씨에 대해 이야기합니다. #Person1#은 소풍을 가고 싶지만, #Person2#는 비가 올 것이라고 생각합니다.',\n",
       " '#Person2#는 참고자료실에서 컴퓨터에 관한 일반적인 정보를 찾고 싶어합니다. #Person1#은 #Person2#에게 참고자료실의 제목을 알려줍니다.',\n",
       " '#Person1#은 프렌치 가든 레스토랑에서 #Person2#를 환영합니다. #Person2#는 #Person1#에게 생수, 주스, 콜라를 주문하고 참치 샌드위치, 야채 수프, 물을 주문합니다.',\n",
       " '#Person2#는 #Person1#에게 레모네이드, 바비큐 윙, 베이비 백 립을 주문합니다.',\n",
       " '#Person2#는 #Person1#에게 커피와 디저트를 저녁식사와 함께 주문하라고 요청했습니다.',\n",
       " '#Person1#은 #Person2#에게 에릭이 로또에 당첨되면 세계 여행을 위한 티켓 두 장을 사줄 것이라고 말한다. #Person2#의 아빠는 놀라서 기절할 것이다.',\n",
       " '잭이 #Person1#에게 자신의 강아지 사진을 보여줍니다. #Person1#은 잭의 아기가 귀엽다고 생각합니다.',\n",
       " '#Person1#은 제인 이모가 톰에게 새 자전거를 사줬다고 #Person1#에게 말한다. #Person1#은 톰이 예의 바르게 행동한다고 생각한다. #Person2#는 언젠가 큰 차를 사줄 것이다.',\n",
       " '#Person2#는 최신 치마를 찾고 있습니다. #Person1#은 #Person2#에게 400달러를 지불합니다.',\n",
       " '#Person2#는 #Person1#에게 #Person2#의 도시는 200년 전만 해도 그저 보잘것없는 작은 마을이었다고 말합니다. #Person2#는 마을의 이름들이 도시의 일부의 이름으로 남아있다고 말합니다.',\n",
       " '#Person1#과 #Person2#는 세계 지도자들이 모여서 행동 계획에 동의하기를 바랍니다. #Person2#는 대기 오염 문제를 해결해야 대기를 파괴하지 않을 수 있다고 생각합니다. #Person1#은 환경 개선 프로젝트에 참여할 수 있으며, 환경 보호에 헌신하는 조직에 가입할 수도 있습니다.',\n",
       " '데니스는 #Person1#에게 채팅방에서 사람들과 많은 시간을 보내고 있다고 말한다. #Person1#은 데니스에게 새로운 온라인 친구가 생겼다고 말한다. 데니스는 데니스에게 좋은 의사를 찾아볼 것이다.',\n",
       " '#Person1#은 네이선에게 시카고로 연습하러 갈 준비가 되었다고 말한다. 네이선은 큰 신문사에서 일하는 것에 대해 걱정하고 있다. #Person1#은 네이선이 사교적이라고 생각한다. 네이선은 #Person1#에게 직장에서의 훈련을 받을 수 있을 것이라고 말한다.',\n",
       " '#Person2#는 3일 동안 토요타 캐롤라를 빌리고 싶어합니다. #Person1#은 #Person2#에게 차 색상을 선택하고 신분증을 복사하라고 요청합니다.',\n",
       " '#Person1#은 사업 계약을 체결하기 위해 뉴욕에 갈 예정입니다. #Person2#는 #Person1#에게 뉴욕의 대학교, 컬럼비아 대학교, 그리고 기차역 근처의 관광객 안내소를 추천합니다.',\n",
       " '#Person1#은 #Person2#에게 필름을 현상해 달라고 요청합니다. #Person2#는 내일 다시 올 것입니다.',\n",
       " '#Person2#는 #Person1#의 비행기가 폭우로 인해 지연되었습니다. #Person1#은 #Person2#에게 최근 비행 안내 방송을 들어보라고 조언합니다.',\n",
       " '#Person2#는 #Person1#에게 지도에서 베이징 대학교로 가는 방법을 알려줍니다.',\n",
       " '#Person1#은 #Person2#에게 좋은 아침이라고 말합니다. #Person2#는 #Person1#에게 일기 예보를 알려줍니다. #Person1#은 은행 강도 두 명을 잡았지만, 그들이 범인이 아니라고 말합니다.',\n",
       " '#Person1#은 컴퓨터 게임을 싫어한다. #Person2#는 #Person1#에게 소년들이 컴퓨터 게임을 통해 컴퓨터를 사용하는 법을 배울 수 있다고 말한다.',\n",
       " '#Person1#은 짐에게 저녁 식사 후에 맥주를 마시자고 제안한다. 짐은 #Person1#에게 운동장에 가서 노래를 부르고 친구들과 춤을 추자고 제안한다. #Person1#은 동의한다.',\n",
       " '#Person1#과 #Person2#는 닭발을 주문하고, #Person2#는 와인을 주문한다.',\n",
       " '#Person1#은 #Person2#에게 안내 경험이 많지는 않다고 말합니다. #Person2#는 중국에서 가장 큰 황과수 폭포를 보여줬다고 말합니다.',\n",
       " '#Person1#은 잭의 더블룸을 예약하는 데 도움을 줍니다.',\n",
       " '질이 마크에게 전화를 걸어 데이비드의 생일 파티에 대해 묻습니다. 마크는 질에게 데이비드의 아내가 어제 밤에 딸을 낳았다고 말합니다. 데이비드는 내일 다시 술을 마실 예정입니다.',\n",
       " '#Person1#은 #Person2#에게 극장에 가자고 제안하지만 #Person2#는 거절한다. #Person1#은 남편이 좋아하지 않을 것이라고 생각한다.',\n",
       " '#Person1#과 #Person2#는 야구 경기를 보기 위해 핫도그와 맥주를 마시고 있습니다. #Person1#은 볼티모어가 이기고 있다고 생각합니다.',\n",
       " '#Person1#은 #Person2#에게 도움을 청합니다. #Person2#는 #Person1#에게 친구 같다고 말합니다.',\n",
       " '#Person1#은 #Person2#에게 #Person2#의 나라에서 천연자원을 수입하는 방법에 대해 묻습니다. #Person2#는 #Person1#에게 정부가 #Person2#의 나라의 사회 기반 시설에 투자해야 한다고 말합니다.',\n",
       " '#Person1#과 #Person2#는 예술 전시회에 대해 이야기하고 있습니다. #Person1#은 내일 국립 갤러리에서 열리는 전시회에 #Person2#를 초대합니다. #Person2#는 조각을 좋아하고 #Person1#은 조각을 좋아합니다.',\n",
       " '#Person2#는 책을 반납하고 비디오를 대출하고 싶어합니다. #Person1#는 #Person2#에게 비디오를 잘 관리해달라고 요청합니다.',\n",
       " '#Person1#은 #Person2#에게 볼링 게임에 대해 더 자세히 알려달라고 요청합니다.',\n",
       " '#Person2#는 #Person1#에게 은행과 증권사 간에 송금할 수 있는 새로운 서비스에 대해 이야기합니다. #Person1#는 맞춤형 상담 서비스를 제공하고, 전화로 상담 서비스를 이용할 수 있다고 말합니다.',\n",
       " '#Person1#은 30분 후에 체크아웃할 예정입니다. #Person2#는 #Person1#에게 짐을 둘 곳이 필요하다고 말합니다. #Person1#은 #Person2#에게 VISA를 제시하여 보증금을 지불하라고 제안합니다.',\n",
       " '#Person2#는 #Person1#에게 파티에 몇 명이 오는지 알려줍니다. #Person1#은 #Person2#에게 뷔페와 차가운 요리, 탄산음료에 대한 추가 요금을 알려줍니다.',\n",
       " '#Person2#는 내년에 해외에서 공부할 예정입니다. #Person1#는 #Person2#에게 적합한 대출에 대한 정보를 제공합니다. #Person2#는 #Person1#에게 나이 제한이 없다고 말합니다. #Person1#는 동의합니다.',\n",
       " '#Person1#은 #Person2#에게 영어 노래가 있는지 묻습니다. #Person2#는 그것들을 찾는 데 몇 분 더 걸릴 수 있다고 말합니다.',\n",
       " '#Person2#는 #Person1#에게 #Person2#가 신문사에서 글쓰기 경험을 가지고 있다고 말합니다.',\n",
       " '잭은 면접 결과를 적극적으로 물어보는 것이 최선이라고 생각한다. #Person1#은 문의서를 간결하게 작성하라고 잭에게 조언한다. 잭은 동의한다.',\n",
       " '#Person2#는 #Person1#에게 #Person2#의 집을 둘러보러 오라고 요청합니다. 그들은 나무에 매달린 옥수수 이삭에 대해 이야기합니다.',\n",
       " '#Person1#은 40분 후에 호텔에서 체크아웃할 예정입니다. #Person2#는 #Person1#에게 보증금을 납부하고 보관소에 수하물을 맡길 수 있다고 말합니다. #Person1#은 체크아웃 시간이 별로 남지 않았다고 말합니다.',\n",
       " '브랜든은 새로운 웹사이트에 가입하려고 한다. #Person1#은 브랜든에게 무료로 글쓰기 기술을 향상시켜 준다고 말한다. #Person1#은 이것이 안전한 URL 같지 않다고 생각한다. #Person2#는 브랜든의 동생이 사기에 속는 것을 볼 수 없다고 생각한다.',\n",
       " '#Person1#과 #Person2#는 새로운 실험실 건물에 대해 논의하고 있습니다. #Person1#은 교장이 지방 정부에게 새 건물을 지을 수 있도록 도움을 청할 것이라고 믿습니다.',\n",
       " '헨리는 #Person1#에게 케이트가 은행에서 준 신용카드를 가지고 다니게 된 것이 기쁘다고 말한다. 왜냐하면 케이트는 카드로 결제한 물건들을 잃어버릴 수 있기 때문이다.',\n",
       " '스미스씨는 #Person1#에게 내일 계획된 마을 방문에 대해 설명합니다. #Person1#은 산악 지역의 마을을 방문하는 것을 제안합니다. 스미스씨는 내일 조금 일찍 일어나야 합니다.',\n",
       " '벳은 #Person1#에게 아기를 가지게 되면서 목표나 꿈을 이루는 데 방해가 되었다고 말한다. 벳은 치어리더가 되고 싶고, 변호사가 되고 싶고, 유타를 떠나고 싶고, 가장 중요한 고등학교 졸업과 결혼을 계획하고 있다. 벳은 청소년들에게 조언을 하고 싶다.',\n",
       " '#Person2#는 #Person1#에게 9년 동안 우표 수집에 관심을 가지게 되었다고 말합니다. #Person2#는 첫 우표에 퀸 빅토리아의 사진이 그려져 있다고 말합니다.',\n",
       " '톰은 #Person1#에게 그녀의 가족들이 성공한 사업가들이라고 말합니다. 톰은 현재 회사를 위한 자금은 약 50명의 사람들로부터 1800만 달러를 모았습니다. #Person1#은 톰에게 그가 낭비적인 일을 한 적이 있다고 생각하며, 그가 성장할 기회를 준다고 말합니다.',\n",
       " '#Person1#과 #Person2#는 니스든의 보행자 지하도에서 발견된 40대로 추정되는 남자의 죽음에 대해 이야기하고 있습니다. 존 데이 형사 부장은 사망 원인이 다발성 두부 손상으로 밝혀졌다고 말합니다. 존 데이 부장은 #Person1#에게 그가 니스든 골목길의 레벨 원 클럽을 떠났다고 말합니다. #Person1#은 존 데이에게 그들의 간격에 대해 묻습니다. 존 데이 부장이 #Person1#의 질문에 답합니다.',\n",
       " '밥은 친구의 결혼식에 참석하기 위해 내년 여름에 퀘벡을 방문할 예정이다. 셰리는 #Person1#에게 퀘벡의 주와 몬트리올의 수도에 대해 설명한다. 밥은 퀘벡이 세계에서 세 번째로 큰 프랑스어 사용 도시이기 때문에 가기 전에 프랑스어를 연습하는 것을 제안한다.',\n",
       " '톰과 제인은 학생 센터에서 수영하고 저녁 식사를 할 예정이다. 톰은 먼저 집에 가고 제인은 그릴에서 저녁 식사를 할 것이다. 그들은 6시쯤 만나기로 한다.',\n",
       " '#Person1#은 등에 통증이 있다. #Person2#는 #Person1#에게 약을 복용하라고 제안한다.',\n",
       " '#Person1#은 인터넷에서 새 컴퓨터를 샀는데 할부를 내야 해서 프레드에게 200 위안을 빌리려고 한다. #Person1#은 동의한다.',\n",
       " '#Person1#은 #Person2#의 글씨를 개선하고 싶어합니다. #Person2#는 #Person1#에게 인내심을 가지라고 조언합니다. #Person1#은 그것이 긴 과정이라고 생각합니다.',\n",
       " '#Person2#는 #Person1#에게 속눈썹을 집고 있다고 말한다. #Person1#는 #Person2#가 겁쟁이라고 생각한다.',\n",
       " '#Person2#는 주말 운전 수업에 대해 #Person1#에게 알려줍니다. #Person1#는 #Person2#에게 하루에 3시간씩 연수를 받고, 각 차량마다 코치 2명이 배정됩니다.',\n",
       " '티나는 8년 동안 피아노를 배웠다. #Person1#은 티나에게 그 선생님을 소개해 줄 것이다.',\n",
       " '#Person1#은 #Person2#에게 #Person2#의 장점과 약점에 대해 묻습니다. #Person2#는 이 일에 대해 배울 수 있는 모든 것에 대해 매우 관심이 있습니다. #Person1#은 5년 후에 #Person2#가 어디에 있을지 궁금해합니다. #Person2#는 #Person1#을 위로합니다.',\n",
       " '스테파니는 젠킨스 선생님이 다음 주 월요일 회의에 필요하다고 해서 내일 보고서를 제출해야 한다. #Person1#은 스테파니에게 의사에게 가보라고 제안하고, 스테파니는 동의한다.',\n",
       " '#Person1#과 #Person2#는 크리스마스에 무엇을 할지에 대해 이야기하고 있다. #Person1#의 아빠는 바바라를 크리스마스 파티에 초대했다. #Person2#는 조용한 크리스마스가 좋다고 생각한다.',\n",
       " '밥은 친구를 만나러 갔다가 댄스 파티에 갔고, 일요일에는 테니스를 쳤다. #Person1#은 이번 주말에 밥과 게임을 하기로 했다.',\n",
       " '#Person1#은 #Person2#에게 50달러를 빌리려고 하지만 #Person2#는 일자리가 없다. #Person2#는 #Person1#에게 #Person1#이 좋아하는 일을 찾는 것이 행운이라고 말한다.',\n",
       " '#Person1#은 #Person2#의 재킷과 넥타이를 빌리고 싶어합니다. #Person2#는 #Person1#의 편의를 위해 회의 시간을 9시 30분으로 미룰 것입니다.',\n",
       " '#Person2#는 #Person1#에게 아이들이 휴가 캠프와 홀리로드를 방문했다고 말합니다. #Person2#는 여행, 가이드 투어, 그리고 바베큐 파티를 포함하여 아이들이 가장 좋아하는 운동을 했다고 말합니다.',\n",
       " '#Person1#은 #Person2#에게 대학 입학 요건에 대해 묻습니다. #Person2#는 직장을 그만두고 컴퓨터 프로그래밍 쪽으로 진출하고 싶어합니다. #Person1#은 컴퓨터 전공자로서 도움을 줄 수 있다고 생각합니다.',\n",
       " '칼리나는 #Person1#에게 클라크 교수님이 차를 나무에 박아서 몇 일 동안 학교를 쉬어야 한다고 말한다. #Person1#은 그에게 이번 주만 쉬면 된다고 말한다.',\n",
       " '#Person1#은 #Person2#에게 집주인이 너무 오래 걸려서 수리를 하지 않는다고 말합니다. #Person2#는 집주인이 불평하고 있던 고장난 세탁기를 고치고, 수리 비용을 제외하고 집에 대한 돈을 우편으로 보냈지만, 집주인이 화를 내며 전화해서 수리 비용을 지불하지 않겠다고 했습니다.',\n",
       " '#Person2#는 터너 인테리어에서 수출용 L/C를 수령하기 위해 사유리 베드에 가야 합니다. #Person1#은 #Person2#에게 서명만 해달라고 요청합니다.',\n",
       " '미르달은 찰리의 지갑을 잃어버렸다. 찰리는 미르달에게 핫도그 판매대로 가자고 제안한다.',\n",
       " '#Person1#과 #Person2#는 번갈아 설거지를 하기로 합의했습니다.',\n",
       " '#Person1#과 #Person2#는 이 관계가 어디로 가고 있는지 알고 싶어한다. #Person2#는 #Person1#이 대단하다고 생각한다. #Person1#은 #Person2#를 사랑한다.',\n",
       " '#Person1#과 #Person2#는 정부가 많은 사회 문제에 직면해 있다고 생각합니다.',\n",
       " '#Person2#는 #Person1#에게 파티에 참석하기 위해 무엇을 해야 할지 모른다고 말한다. #Person1#는 #Person2#에게 캐나다, 캐나다 경찰 복장을 어디서 구할 수 있는지 알려준다. #Person2#는 동의한다.',\n",
       " '#Person1#은 #Person2#에게 춤추러 가자고 제안합니다. #Person2#는 동의하고 디스코 댄스를 제안합니다. #Person1#은 동의하고 다음 댄스는 쉬기로 합니다.',\n",
       " '#Person1#은 사무실에서 9시부터 5시까지 일하는 것에서 벗어나고 싶어한다. #Person2#는 농부가 되기 전에 많은 훈련이 필요하다고 생각한다. #Person1#은 #Person2#를 찾아갈 것이다.',\n",
       " '#Person1#은 #Person2#에게 하이네켄과 버드 반 파인트, 나초, 모짜렐라 스틱을 주문합니다.',\n",
       " '메리는 #Person1#에게 어제 밤에 앤과 큰 싸움을 했다고 말한다. 앤은 메리에게 그녀의 우정을 전혀 신경 쓰지 않는다고 말했고, 메리는 화가 났다. 그들은 나중에 화해할 것이다.',\n",
       " '#Person2#는 #Person1#에게 1층에 있는 가게에서 담배를 사고 기념품도 구입할 수 있다고 말합니다.',\n",
       " \"캐서린은 '패스트 푸드 네이션'이라는 영화가 올 아메리칸 식사의 어두운 면을 보여준다고 생각한다. 톰은 패스트푸드가 편리함을 갈망하는 라이프스타일이라고 생각한다. 그들은 패스트푸드 레스토랑에서도 건강한 메뉴 옵션을 제공하는 것에 동의한다.\",\n",
       " '#Person2#는 #Person1#에게 버스가 그린위치 빌리지까지 피프스 애비뉴를 따라 가는 것이 아니라고 말합니다. #Person1#는 #Person2#가 워싱턴 스퀘어 파크로 가는 올바른 버스를 선택하도록 도와줍니다.',\n",
       " '#Person2#는 9-11 테러 공격 당시 베이징의 아파트에 있었습니다. #Person2#는 #Person1#에게 테러범들이 쌍둥이 타워를 파괴했다고 말합니다. #Person1#는 테러범들이 인간의 생명을 전혀 존중하지 않았다고 생각합니다.',\n",
       " '척 존스가 칼에게 이웃이 되었다고 말합니다. 칼은 시카고 출신이지만 이웃들과 친해지려는 사람들이 별로 없다고 말합니다.',\n",
       " '#Person1#은 #Person2#에게 계란과 토스트, 그리고 오렌지 주스를 주문하는 데 도움을 줍니다.',\n",
       " '스티븐은 피곤해서 침대에 들어간다. #Person1#은 스티븐에게 가계 예산을 살펴봐야 한다고 말한다. 스티븐은 졸리지 않다.',\n",
       " '#Person1#과 제인은 다음 주 월요일 오후 세 시쯤 만나기로 결정했다.',\n",
       " '#Person2#는 #Person1#의 여자친구에게 줄 좋은 선물을 찾고 있습니다. #Person1#은 #Person2#에게 그녀에게 진주 귀걸이 세트나 아름다운 하트 모양 펜던트를 추천합니다. #Person2#는 또한 #Person1#에게 제시에게 반지와 목걸이를 선물하는 것을 제안합니다.',\n",
       " '#Person1#은 오래된 포드 핀토를 매입하고 싶어합니다. #Person2#는 #Person1#에게 그것이 매우 가볍지만 강력한 강력한 차량이라고 말합니다. #Person1#은 #Person2#에게 이 차를 0% 다운페이먼트와 이자 없이 가져갈 수 있다고 말합니다.',\n",
       " '#Person2#는 발람씨에게 컴퓨터 엔지니어 직위를 제안하고 월 4,000 위안의 급여를 제공합니다. 발람씨는 가족을 부양하기 위해 월급 4,000 위안을 받기로 결정합니다.',\n",
       " '#Person2#는 #Person1#에게 통합 자금 이체와 중앙 집중식 배분을 제공하는 네트워크 결제 서비스에 가입하도록 권장합니다. #Person1#는 동의합니다.',\n",
       " '#Person1#은 배가 고프다. #Person2#는 #Person1#에게 저녁으로 샌드위치를 만들어 먹으라고 제안한다.',\n",
       " '#Person1#은 #Person2#에게 피를 뽑으라고 요청하고, 혈액을 풍부하게 만들기 위해 터니켓을 달아줄 것입니다.',\n",
       " '스티븐은 전기가 나가서 #Person1#에게 지하로 내려가서 회로 상자를 열어보고 탄 퓨즈를 풀어서 좋은 것으로 교체하라고 조언한다.',\n",
       " '파울라는 #Person1#에게 빌린 집의 집주인이 임대료에서 차감된 비용을 지불하지 않았다고 위협하고 있다고 말한다. #Person1#은 파울라에게 문제를 해결하는 데 도움을 주겠다고 약속한다.',\n",
       " '#Person1#은 에어컨이 작동하지 않아 #Person2#에게 수리공을 불러야 한다고 말합니다. #Person2#는 #Person1#에게 전기 배선도 고치라고 요청합니다.',\n",
       " '#Person1#은 해고당한 후 일자리를 찾고 있다. #Person2#는 전기기사 수습생 프로그램 공고에 대해 #Person1#에게 이야기한다.',\n",
       " '#Person2#는 #Person1#에게 사장님이 어려운 상황에서도 올바른 결정을 내리는 데 뛰어나다고 말합니다.',\n",
       " '#Person1#은 탕 씨에게 투어 가이드 자격증을 요청합니다. 탕 씨는 두 가지 언어와 영어와 러시아어를 할 수 있습니다.',\n",
       " '#Person1#은 #Person2#에게 시계를 보여줍니다. #Person2#는 가격이 너무 비싸다고 생각하지만, #Person1#은 가격이 적당하다고 생각합니다.',\n",
       " '톰이 사라씨에게 전화를 걸어 딸 마리아가 고열이 나서 곧 병원에 데려갈 것이라고 말한다. 사라씨는 #Person1#에게 저녁을 같이 먹고 켄과 게임을 하자고 제안한다.',\n",
       " '에이미는 #Person1#에게 첫 직장이 어디였는지 설명합니다. 에이미는 영업 부서의 직위에 지원하고 싶어서 그만뒀습니다. 지금은 그곳에서 일하고 있습니다.',\n",
       " '앤드류는 #Person1#에게 자신이 뚱뚱하다고 말한다. #Person1#은 앤드류에게 어제부터 삶을 바꾸기 시작했다고 말한다. 앤드류는 와푸 다이어트를 하고 있고, #Person1#은 #Person1#이 사기를 당하고 있다고 생각한다. 앤드류와 #Person1#은 아침 식사를 건너뛰고 점심과 저녁에 과식하고 많은 간식을 먹어야 한다고 생각한다. #Person1#는 앤드류가 더 이상 뚱뚱하지 않다고 생각한다. #Person2#는 앤드류를 위로한다. 앤드류는 앤드류에 대해 이야기하고 앤드류가 제대로 먹는 법을 배워야 한다고 생각한다. 그는 또한 앤드류에 대한 몇 가지 조언을 한다.',\n",
       " '브라운 대학의 그렉 손더스씨가 메리에게 전화를 걸어 그녀의 성적 평균과 대학 스포츠에 대해 묻습니다. 메리는 지원서에 농구를 했다고 썼고 배구를 합니다.',\n",
       " '#Person1#은 #Person2#의 택시를 타고 기차역으로 가고 있습니다. #Person2#는 #Person1#에게 천천히 조심해서 운전하라고 말합니다.',\n",
       " '브라이언은 #Person1#에게 대학에서 영어를 배웠다고 말한다. #Person1#은 캘리포니아에 가본 적이 없다고 말한다.',\n",
       " '#Person1#은 새로운 건강보험에 가입하기 위해 연간 건강검진을 받으러 왔습니다. #Person2#는 #Person1#에게 기본적인 건강검진 정보를 알려줍니다. #Person1#은 #Person2#의 건강을 챙깁니다.',\n",
       " '#Person1#은 #Person2#에게 복권에 당첨되면 세계 일주를 할 것이라고 말하지만, #Person2#는 세계 일주를 하는 데 오랜 시간이 걸릴 것이라고 말한다. #Person2#는 #Person1#에게 스카치를 마시자고 제안하지만 #Person1#은 거절하고 #Person2#는 맥주를 마시자고 제안한다.',\n",
       " '#Person1#은 #Person2#에게 자신이 아직 35살이라고 말한다. #Person2#는 #Person1#이 싱글 생활에 지쳤다고 생각하고 에이미를 떠날 수 없다고 말한다. #Person1#은 에이미에게 이성을 되찾아주어야 할 것 같다고 말한다.',\n",
       " '#Person1#은 2,000장의 명함을 인쇄해야 합니다. #Person2#는 이전에 사용하던 명함과 똑같이 만들어 달라고 요청합니다. #Person1#은 동의합니다.',\n",
       " '브라이언은 #Person1#에게 체크인을 하고 7시까지 체크인을 해야 한다고 말합니다. 회의는 정오에 시작합니다. 브라이언은 업무 시작 전에 프로그램을 준비해 줄 것입니다.',\n",
       " '#Person1#은 추수감사절 저녁 식사에 폴을 다시 초대한다. 폴은 #Person1#에게 다음 주에 해야 할 일이 많다고 말한다. #Person1#은 폴을 초대하고 파티에 초대하지만 폴은 여동생이 이미 디저트를 맡았다고 말한다.',\n",
       " '수잔은 #Person1#에게 존의 사촌이 대학에 면접을 보러 올 예정이며, 토요일에는 도서관에서 근무해야 하기 때문에 나갈 수 없다고 말한다. #Person1#은 눈보라가 오면 수잔에게 전화를 걸어 그녀를 데려다 줄 것이다.',\n",
       " '벤은 #Person1#에게 꽃꽂이가 지루할 수 있다고 말한다. #Person1#은 #Person2#에게 인도 요리를 추천하지만 #Person2#는 비싸다고 생각한다. 결국, #Person1#은 인도 요리를 시도하기로 결정한다.',\n",
       " '#Person1#과 #Person2#는 오늘 저녁에 피자 체험에 가서 식사하기로 결정했다. #Person1#은 브리짓과 킹피셔를 제안하지만 #Person2#는 목요일에 열지 않기 때문에 거절한다.',\n",
       " '진은 #Person1#에게 이번 토요일 아침에 운전 면허 시험을 볼 것이라고 말한다. 진은 시험을 통과하면 2016년형 혼다 어코드를 사려고 계획하고 있다.',\n",
       " '#Person1#은 #Person2#에게 펜을 보여줍니다. #Person2#는 #Person1#에게 카드를 사용하라고 요청합니다.',\n",
       " '#Person2#는 #Person1#에게 회사의 기금 모금 행사는 재미로 몇몇 경쟁사들과 파트너십을 맺어 미국 암 협회를 위한 마라톤을 후원했다고 말합니다. #Person2#는 이것이 미국 암 협회에게는 좋은 일이라고 생각합니다.',\n",
       " '#Person1#은 #Person2#에게 피크 트램까지 어떻게 가는지 알려줍니다. #Person2#는 #Person1#에게 퀸스 로드를 따라 가고, 힐튼에서 오른쪽으로 꺾고, 가든 로드를 따라 올라가라고 말합니다.',\n",
       " '#Person1#은 #Person2#에게 샌들우드 부채와 작은 것 두 개를 사는 데 도움을 줍니다.',\n",
       " '#Person2#는 #Person1#에게 회사의 관점에 영향을 미치는 외부 요인은 인적 자원, 팀워크, 혁신 정신이라고 말합니다.',\n",
       " '#Person1#은 주제를 다루기 위해 다시 모였으면 좋겠다고 #Person2#에게 요청합니다. #Person2#는 오늘 오후 2시에 준비 회의를 시작할 것입니다.',\n",
       " '#Person2#는 #Person1#에게 #Person2#의 도시에서 하루만 머무르는 짧은 투어를 추천합니다.',\n",
       " '#Person1#은 #Person2#가 소고기 버거, 프렌치 프라이, 그리고 밀크 쉐이크를 주문하는 데 도움을 줍니다.',\n",
       " '#Person1#과 #Person2#는 등 축제에 대해 이야기하고 있습니다. #Person1#은 #Person2#에게 등에 적힌 글자를 읽고 중국어 글자를 읽을 줄 안다고 말합니다. #Person2#는 #Person1#에게 다른 것을 시도해 보라고 제안합니다.',\n",
       " '티나는 #Person1#에게 ABC 컴퍼니와의 면접에 성공적으로 통과했다고 말한다. #Person1#은 티나에게 축하 파티를 제안하고 티나는 동의한다.',\n",
       " '팀은 카렌에게 다시 만날 수 있기를 바란다.',\n",
       " '#Person1#과 #Person2#는 10시에 부서 회의가 있다. #Person2#는 몇 가지 문구를 준비하고, 줄리에게 몇 장의 사본을 만들고, 사무실에서 일하는 것을 즐긴다. #Person1#은 #Person2#가 친절하다고 생각한다.',\n",
       " '#Person1#은 버튼이 너무 많아서 목욕을 하려고 합니다. #Person2#는 #Person1#에게 어떻게 사용하는지 설명합니다.',\n",
       " '#Person1#은 #Person2#에게 기차에서 시간을 보내기 위한 책에 대한 조언을 구합니다. #Person2#는 소설을 좋아하고 #Person1#은 일본 소설을 좋아합니다.',\n",
       " '#Person2#는 #Person1#에게 #Person2#의 친구가 78살이고 남미에서 새로 이민 온 사람이라고 말합니다. #Person1#는 #Person2#가 읽는 것을 가르치는 것에 능숙할 것이라고 생각합니다.',\n",
       " '#Person1#은 #Person2#의 티켓을 찾으러 왔습니다. #Person2#는 #Person1#에게 티켓 예약 양식을 작성하라고 요청합니다.',\n",
       " '#Person1#은 #Person2#에게 냉동 피자를 오븐에 넣는 것을 제안하고, #Person2#는 건강한 음식을 만드는 법을 가르칠 수 있다고 생각한다. #Person2#는 #Person1#에게 채소를 씻은 다음 잘게 썰고 볶음용 팬을 데우라고 조언한다. #Person1#은 또한 #Person2#에게도 치킨 커리 레시피를 보여준다.',\n",
       " '#Person2#는 #Person1#에게 부스에 있는 두 대의 전화기가 다르다고 말합니다. #Person1#은 #Person2#에게 IC 전화기와 동전 전화기 사용법을 알려줍니다.',\n",
       " '톰은 돈이 부족해서 중고 컴퓨터를 사려고 생각하고 있다. #Person1#은 그에게 중고 물품이 새 것만큼 좋다고 말한다.',\n",
       " '#Person1#은 모건에게 중국 사람들이 식당에서 남은 음식을 어떻게 가져가는지 물어본다. 모건은 동의한다. #Person1#은 #Person1#에게 남은 음식을 집으로 가져가는 것이 더 합리적이라고 말한다.',\n",
       " '해리는 #Person1#에게 중국 거리 시장에서 절대로 쇼핑하지 않을 것이라고 말합니다. #Person1#은 흥정하는 것이 항상 거래에서 빠질 수 없는 규칙이라고 말합니다. 해리는 중국 동료들이 그 가방을 너무 비싸게 샀다고 말했다고 전합니다. 해리는 흥정을 하는 것이 짜증나는 일이라고 생각합니다.',\n",
       " '#Person1#은 #Person2#에게 영어를 능숙하게 구사해야 한다고 말합니다. #Person2#는 프랑스어와 일본어를 할 줄 안다고 말합니다.',\n",
       " '#Person1#은 새로운 계좌를 개설하고 싶어합니다. #Person2#는 #Person1#에게 당좌 계좌와 저축 계좌를 모두 개설하고 초과 인출에 대한 벌금 없이 1%의 이자만 내면 된다고 말합니다.',\n",
       " '#Person2#는 #Person1#에게 더 많이 구매하면 더 많은 특별 할인 쿠폰을 받을 수 있다고 말합니다. #Person1#은 #Person2#에게 유효기간 내에 사용할 수 있다고 말하고, #Person2#는 쿠폰 3장을 받을 것입니다.',\n",
       " '로빈슨 부인이 스티브에게 쟈니를 돌봐주고 접시를 닦아줬다고 말한다.',\n",
       " '스미스는 상하이로 가기 위해 기차 표를 잃어버렸습니다. #Person1#은 스미스에게 다시 표를 사라고 요청합니다. 스미스는 #Person1#에게 편안한 좌석을 원하면 610 RMB를 사용하고, 딱딱한 좌석에서는 100RMB를 사용한다고 말합니다. 스미스는 지갑을 훔쳤습니다. 스미스는 지갑에 돈이 있는지 확인하기 위해 다시 찾아갑니다.',\n",
       " '앤은 다음 주에 뉴욕으로 여행을 갈 예정이다. #Person1#은 앤에게 항공 웰빙 프로그램을 추천한다. 앤은 비행 중에 그 프로그램을 했고, 뉴욕에 도착했을 때는 시차 적응도 전혀 없었다고 말한다. 앤은 샴페인과 시차 적응을 위한 몇 가지 운동을 했다고 말한다.',\n",
       " '톰이 메리에게 존의 파티에 가자고 제안하지만 메리는 일자리를 찾았다. 메리는 맥도날드에서 부점장이지만 다른 일을 찾고 싶어한다. 톰은 그에게 전화할 것이다.',\n",
       " '#Person1#은 해리에게 끔찍한 경험을 했다고 말한다. 해리는 #Person1#에게 그런 운전자들은 처벌을 받아야 한다고 말한다. #Person1#은 동의한다.',\n",
       " '#Person1#은 자전거 상점을 인수한 스티븐 케인씨를 인터뷰합니다. 케인씨는 자신의 취미를 즐길 시간이 거의 없었기 때문에 항상 레이싱 자전거를 타고 수리하는 것을 좋아했습니다. 케인씨는 또한 함께 일할 직원을 고용했습니다.',\n",
       " '#Person1#과 #Person2#는 좋은 중년 부부입니다. #Person2#는 #Person1#에게 그들이 중년이 아니라고 말합니다. #Person1#은 동의합니다.',\n",
       " '#Person1#과 #Person2#는 멋진 식사를 마무리하고 있다. #Person2#는 친구들과 나눠 먹고 싶어한다. #Person1#은 사과 크리스프와 초콜릿 무스 케이크를 주문하고, #Person2#는 커피와 차를 주문한다.',\n",
       " '#Person1#은 #Person2#에게 아래쪽 침대를 원한다고 말한다. #Person2#는 #Person1#에게 스테레오와 카운터 위에 스테레오를 놓고, 창가 옆의 책상을 가져가라고 제안한다. #Person1#은 동의한다. #Person2#는 또한 여자들과 관련해서 엄청나게 운이 좋다고 말한다. #Person1#은 또한 #Person2#가 초보자들을 도와주는 것을 좋아한다고 말한다.',\n",
       " '머레이 씨가 #Person1#에게 도서관 카드를 발급해줍니다. #Person1#은 #Person2#에게 면허증과 신청서를 보여줍니다.',\n",
       " '#Person1#은 #Person2#에게 #Person2#의 근무 일정에 대해 묻습니다. #Person2#는 보통 9시부터 5시까지 일하고 초과근무 수당 없이 보너스를 받습니다. #Person1#은 커피를 마시면서 쉬는 시간을 가지면 짜증을 낸다고 말합니다.',\n",
       " '#Person2#는 #Person1#에게 세탁기, 건조기, 비누, 비누를 어떻게 사용하는지 가르쳐주고, #Person1#의 엄마가 #Person2#의 옷을 빨래해 준다고 말한다. 닉은 #Person2#의 엄마가 되고 싶어한다. #Person2#는 #Person2#에게 #Person2#가 어떻게 혼자서 살 수 있는지 가르쳐줄 것이다.',\n",
       " '#Person2#가 #Person1#에게 바빴다고 말한다. #Person1#은 #Person2#에게 오늘 밤에 전화하라고 말한다.',\n",
       " '케이티는 #Person1#에게 고객이 없을 때 항상 멍하니 서 있다고 말한다. #Person1#은 #Person2#가 다른 일을 하길 바란다고 말한다.',\n",
       " '#Person1#은 다음 주에 할아버지 생신을 위해 호텔에서 깜짝 파티를 할 예정입니다. #Person2#는 #Person1#에게 호텔에서 하는 것이 좋다고 제안하고, #Person1#은 목도리와 모자를 제안합니다.',\n",
       " '로버트는 지안 루카 도나텔리에게 전화를 걸어 이탈리아 서비스업체에서 일하는 지안을 소개합니다.',\n",
       " '#Person1#과 #Person2#는 시골의 새들의 울음소리에 대해 이야기하고 있다. 캐시는 #Person1#에게 새들이 산 근처에서 내려와서 노래를 하고 특별한 춤을 추기 위해 내려온다고 말한다. #Person1#은 도시에는 이런 것이 없다고 생각한다.',\n",
       " '#Person2#는 #Person1#의 도움으로 칠면조 샌드위치를 주문합니다. #Person1#은 #Person2#에게 소고기 스프와 다이어트 콜라를 주문합니다.',\n",
       " '제임스는 #Person1#에게 기차에 탑승하기 위해 짐을 거의 다 싸놨다고 말한다. #Person1#은 제임스가 사진을 찍기 위해 재킷을 입어야 한다고 생각한다. 제임스는 쿠키는 가방에 넣었고, #Person1#은 그를 위해 쿠키를 만들었다.',\n",
       " '테드는 #Person1#에게 올해 휴가를 어디로 갈지 아직 결정하지 못했다고 말한다. #Person1#은 중국에서 몇 주를 보낼 예정이며, 테드는 그곳을 방문하고 싶어한다.',\n",
       " '#Person1#은 아빠에게 오후에 영화관에 가자고 제안한다. #Person1#은 영화를 보고 나서 맥도날드에 가자고 제안하고, 아빠는 동의한다.',\n",
       " '#Person2#는 #Person1#에게 어제 밤에 #Person1#의 집이 털렸다고 말한다. #Person1#는 경찰에 전화할 것이다.',\n",
       " '잭이 찰리에게 집에 와서 비디오 게임을 하자고 제안한다. 찰리는 잭에게 캐릭터를 만드는 게임을 추천한다. 잭은 해본 적이 없다.',\n",
       " '#Person2#는 #Person1#에게 컨트리 음악에 관심을 가지게 된 계기와 라디오 방송국에서 일하기 시작한 계기, 그리고 도서관에서 공부하기 시작한 계기에 대해 이야기합니다.',\n",
       " '#Person1#은 앨리스에게 세탁기, 건조기, 비누를 어떻게 사용하는지 묻습니다. 앨리스는 비누가 없다고 말합니다. #Person1#은 #Person2#에게 비누가 너무 많이 필요하지 않다고 말합니다. 앨리스는 #Person1#에게 기계가 비누를 완전히 제거하지 못하고 너무 많은 거품이 먼지를 가둬서 세균이 쌓이게 만든다고 말합니다. #Person2#는 앨리스의 엄마가 학교에서 좋은 성적을 받기를 원하기 때문에 옷을 빨아본 적이 없다고 말합니다.',\n",
       " '스티브는 계약이 다음 달에 끝나기 때문에 집을 찾고 있다. 매튜는 그녀의 이웃인 다우 부인이 아기를 가지게 돼서 그녀를 돕기로 했다고 말한다. 스티브는 그녀를 방문하고 싶어한다.',\n",
       " '프랭크는 벳시에게 승진하고 친구들 모두를 위한 큰 파티를 열 계획이라고 말한다. 벳시는 파티에 참석하고 싶어한다. 프랭크는 파티에 많은 사람이 올 것이라고 기대하고 있다.']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) 최종 결과 데이터프레임으로 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######  최종 결과를 데이터프레임으로 정리합니다. ###############################################################################\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n#######  최종 결과를 데이터프레임으로 정리합니다. ###############################################################################\")\n",
    "# 최종 결과를 데이터프레임으로 정리합니다.\n",
    "output = pd.DataFrame({\n",
    "    \"fname\": test_data['fname'],  # 파일 이름\n",
    "    \"summary\": preprocessed_summary,  # 전처리된 요약문\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) CSV 파일 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#######   jx7789-kobart_summary_v3-2x75p-l6-n4-b5-ck62290_output.csv  파일명이 완성되었습니다.\n"
     ]
    }
   ],
   "source": [
    "print(\"#######  \", model.replace(\"/\", \"-\") + para + \"-ck\" + checkpoint_num + \"_output.csv\" + \"  파일명이 완성되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######  결과를 저장할 경로를 설정하고, 결과를 CSV 파일로 저장합니다.  ##########################################################\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n#######  결과를 저장할 경로를 설정하고, 결과를 CSV 파일로 저장합니다.  ##########################################################\")\n",
    "# 결과를 저장할 경로를 설정하고, 결과를 CSV 파일로 저장합니다.\n",
    "result_path = config['inference']['result_path']\n",
    "if not os.path.exists(result_path):\n",
    "    os.makedirs(result_path)  # 경로가 존재하지 않으면 디렉토리를 생성합니다.\n",
    "output.to_csv(os.path.join(result_path, model.replace(\"/\", \"-\") + para + \"-ck\" + checkpoint_num + \"_output.csv\"), index=False)  # 결과를 CSV 파일로 저장합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>더슨 씨는 #Person1#에게 이메일 통신과 공식 메모로 제한되는 내부 메모에 대...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>#Person1#은 #Person2#에게 교통 체증에 걸렸다고 말한다. #Perso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>#Person1#은 케이트에게 마샤와 히어로가 이혼을 신청했다고 말한다. 케이트는 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>브라이언은 #Person1#에게 자신의 생일을 축하하고, #Person1#은 브라이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>#Person2#는 #Person1#에게 올림픽 공원이 크다고 말합니다. #Pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>test_495</td>\n",
       "      <td>잭이 찰리에게 집에 와서 비디오 게임을 하자고 제안한다. 찰리는 잭에게 캐릭터를 만...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>test_496</td>\n",
       "      <td>#Person2#는 #Person1#에게 컨트리 음악에 관심을 가지게 된 계기와 라...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>test_497</td>\n",
       "      <td>#Person1#은 앨리스에게 세탁기, 건조기, 비누를 어떻게 사용하는지 묻습니다....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>test_498</td>\n",
       "      <td>스티브는 계약이 다음 달에 끝나기 때문에 집을 찾고 있다. 매튜는 그녀의 이웃인 다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>test_499</td>\n",
       "      <td>프랭크는 벳시에게 승진하고 친구들 모두를 위한 큰 파티를 열 계획이라고 말한다. 벳...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>499 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        fname                                            summary\n",
       "0      test_0  더슨 씨는 #Person1#에게 이메일 통신과 공식 메모로 제한되는 내부 메모에 대...\n",
       "1      test_1  #Person1#은 #Person2#에게 교통 체증에 걸렸다고 말한다. #Perso...\n",
       "2      test_2  #Person1#은 케이트에게 마샤와 히어로가 이혼을 신청했다고 말한다. 케이트는 ...\n",
       "3      test_3  브라이언은 #Person1#에게 자신의 생일을 축하하고, #Person1#은 브라이...\n",
       "4      test_4  #Person2#는 #Person1#에게 올림픽 공원이 크다고 말합니다. #Pers...\n",
       "..        ...                                                ...\n",
       "494  test_495  잭이 찰리에게 집에 와서 비디오 게임을 하자고 제안한다. 찰리는 잭에게 캐릭터를 만...\n",
       "495  test_496  #Person2#는 #Person1#에게 컨트리 음악에 관심을 가지게 된 계기와 라...\n",
       "496  test_497  #Person1#은 앨리스에게 세탁기, 건조기, 비누를 어떻게 사용하는지 묻습니다....\n",
       "497  test_498  스티브는 계약이 다음 달에 끝나기 때문에 집을 찾고 있다. 매튜는 그녀의 이웃인 다...\n",
       "498  test_499  프랭크는 벳시에게 승진하고 친구들 모두를 위한 큰 파티를 열 계획이라고 말한다. 벳...\n",
       "\n",
       "[499 rows x 2 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과 확인\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
